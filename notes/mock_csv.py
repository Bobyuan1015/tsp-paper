import traceback
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
from typing import Dict, List, Tuple, Any
import warnings
from scipy import stats
from sklearn.metrics import mean_squared_error
import os
from itertools import combinations

warnings.filterwarnings('ignore')


# æ„å»ºçŠ¶æ€å˜é‡æ˜ å°„å…³ç³»
def build_state_combinations():
    """æ„å»ºæ¶ˆèå®éªŒçš„çŠ¶æ€ç»„åˆæ˜ å°„"""
    base_states = ["current_city_onehot", "visited_mask", "order_embedding", "distances_from_current"]

    map_state_types = {}

    # 1. å…¨çŠ¶æ€ï¼ˆåŸºçº¿ï¼‰
    map_state_types['full'] = base_states.copy()

    # 2. ä¾æ¬¡ç§»é™¤ä¸€ç§çŠ¶æ€ï¼ˆ4ç§ç»„åˆï¼‰
    for i, state_to_remove in enumerate(base_states):
        remaining_states = [s for s in base_states if s != state_to_remove]
        map_state_types[f'ablation_remove_{state_to_remove.split("_")[0]}'] = remaining_states

    # 3. ä¾æ¬¡ç§»é™¤ä¸¤ç§çŠ¶æ€ï¼ˆ6ç§ç»„åˆï¼‰
    for i, (state1, state2) in enumerate(combinations(base_states, 2)):
        remaining_states = [s for s in base_states if s not in [state1, state2]]
        key_name = f'ablation_remove_{state1.split("_")[0]}_{state2.split("_")[0]}'
        map_state_types[key_name] = remaining_states

    return map_state_types


# å…¨å±€æ˜ å°„å…³ç³»
map_state_types = build_state_combinations()


class TSPAblationExperimentGenerator:
    """TSPæ¶ˆèå®éªŒæ•°æ®ç”Ÿæˆå™¨"""

    def __init__(self):
        self.algorithms = ['DQN_visited', 'DQN_LSTM', 'Reinforce', 'ActorCritic', 'DQN_order', 'DQN_optimal']
        self.state_types = list(map_state_types.keys())
        self.city_nums = [10, 20, 30, 50]
        self.modes = ['per_instance', 'cross_instance']
        self.train_test_splits = ['train', 'test']
        self.total_instances = 100
        self.train_instances = 80
        self.test_instances = 20
        self.runs_per_instance = 5

        # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
        np.random.seed(42)

    def generate_optimal_distances(self, city_num: int, instance_id: int) -> float:
        """ç”ŸæˆTSPå®ä¾‹çš„æœ€ä¼˜è·ç¦»ï¼ˆåŸºäºåŸå¸‚æ•°é‡çš„ç»éªŒå…¬å¼ï¼‰"""
        np.random.seed(instance_id)  # ç¡®ä¿æ¯ä¸ªå®ä¾‹çš„æœ€ä¼˜è§£å›ºå®š
        base_distance = np.sqrt(city_num) * 10
        variation = np.random.normal(0, 0.1) * base_distance
        return max(base_distance + variation, base_distance * 0.8)

    def generate_state_representation(self, state_type: str, city_num: int, step: int) -> str:
        """æ ¹æ®çŠ¶æ€ç±»å‹ç”Ÿæˆå¯¹åº”çš„çŠ¶æ€è¡¨ç¤º"""
        current_city = np.random.randint(0, city_num)
        visited_mask = np.random.choice([0, 1], size=city_num, p=[0.3, 0.7])

        # è·å–è¯¥çŠ¶æ€ç±»å‹åº”åŒ…å«çš„ç»„ä»¶
        state_components = map_state_types[state_type]

        state = {}

        # æ ¹æ®çŠ¶æ€ç»„ä»¶åˆ—è¡¨æ„å»ºçŠ¶æ€
        if "current_city_onehot" in state_components:
            state['current_city_onehot'] = [1 if i == current_city else 0 for i in range(city_num)]

        if "visited_mask" in state_components:
            state['visited_mask'] = visited_mask.tolist()

        if "order_embedding" in state_components:
            state['order_embedding'] = [step / 100.0] * city_num

        if "distances_from_current" in state_components:
            state['distances_from_current'] = np.random.exponential(2, city_num).tolist()

        return json.dumps(state)

    def calculate_state_performance_impact(self, state_type: str, algorithm: str, city_num: int) -> Dict[str, float]:
        """æ ¹æ®æ¶ˆèå®éªŒç†è®ºè®¡ç®—çŠ¶æ€ç±»å‹å¯¹æ€§èƒ½çš„å½±å“"""
        # åŸºç¡€æ€§èƒ½å‚æ•°
        algorithm_performance = {
            'DQN_visited': 0.75, 'DQN_LSTM': 0.80, 'Reinforce': 0.70,
            'ActorCritic': 0.85, 'DQN_order': 0.78, 'DQN_optimal': 0.90
        }

        # ç»„ä»¶é‡è¦æ€§æƒé‡ï¼ˆåŸºäºTSPé¢†åŸŸçŸ¥è¯†ï¼‰
        component_importance = {
            'current_city_onehot': 0.30,  # å½“å‰ä½ç½®ä¿¡æ¯ï¼ŒåŸºç¡€ä¸”å…³é”®
            'visited_mask': 0.35,  # è®¿é—®çŠ¶æ€ï¼Œé¿å…é‡å¤è®¿é—®
            'order_embedding': 0.20,  # è®¿é—®é¡ºåºï¼Œå½±å“è·¯å¾„ä¼˜åŒ–
            'distances_from_current': 0.15  # è·ç¦»ä¿¡æ¯ï¼Œå±€éƒ¨ä¼˜åŒ–è¾…åŠ©
        }

        # è®¡ç®—çŠ¶æ€ç»„åˆçš„æ€»é‡è¦æ€§
        state_components = map_state_types[state_type]
        total_importance = sum(component_importance[comp] for comp in state_components)

        # è§„æ¨¡æƒ©ç½š
        scale_penalty = 1 - (city_num - 10) * 0.015
        base_performance = algorithm_performance[algorithm] * scale_penalty

        # æ ¹æ®ç»„ä»¶é‡è¦æ€§è°ƒæ•´æ€§èƒ½
        performance_multiplier = total_importance
        performance = base_performance * performance_multiplier

        return {
            'base_optimality_gap': (1 - performance) * 100,
            'convergence_factor': performance_multiplier,
            'stability_factor': 1 / max(performance_multiplier, 0.1),
            'component_importance': total_importance
        }

    def generate_episode_data(self, algorithm: str, city_num: int, mode: str,
                              state_type: str, instance_id: int, run_id: int,
                              train_test: str) -> List[Dict]:
        """ç”Ÿæˆå•ä¸ªå®ä¾‹çš„episodeæ•°æ®"""
        data = []
        optimal_distance = self.generate_optimal_distances(city_num, instance_id)
        optimal_path = f"optimal_path_{instance_id}"

        # æ ¹æ®çŠ¶æ€ç±»å‹å’Œç®—æ³•è®¾å®šepisodeæ•°é‡
        max_episodes = 100 if train_test == 'train' else 50
        state_value = map_state_types[state_type]
        performance_params = self.calculate_state_performance_impact(state_type, algorithm, city_num)

        np.random.seed(instance_id * 1000 + run_id * 100)

        for episode in range(max_episodes):
            episode_length = city_num
            episode_reward = 0
            current_distance = 0

            for step in range(episode_length):
                # ç”ŸæˆçŠ¶æ€è¡¨ç¤º
                state = self.generate_state_representation(state_type, city_num, step)

                # æ¨¡æ‹Ÿå¥–åŠ±å’ŒæŸå¤±
                step_distance = np.random.exponential(2)
                current_distance += step_distance

                # å¥–åŠ±è®¡ç®—
                step_reward = -step_distance
                if step == episode_length - 1:
                    step_reward -= np.random.exponential(3)

                episode_reward += step_reward

                # æŸå¤±è®¡ç®—
                loss = np.random.exponential(0.5) if algorithm.startswith('DQN') else np.nan

                # æ ¹æ®æ€§èƒ½å‚æ•°è°ƒæ•´æœ€ç»ˆè·ç¦»
                optimality_gap = performance_params['base_optimality_gap']
                final_distance = optimal_distance * (1 + optimality_gap / 100)

                data.append({
                    'algorithm': algorithm,
                    'city_num': city_num,
                    'mode': mode,
                    'instance_id': instance_id,
                    'run_id': run_id,
                    'state_type': state_type,
                    'train_test': train_test,
                    'episode': episode,
                    'step': step,
                    'state': state,
                    'done': 1 if step == episode_length - 1 else 0,
                    'reward': step_reward,
                    'loss': loss,
                    'total_reward': episode_reward if step == episode_length - 1 else np.nan,
                    'current_distance': final_distance if step == episode_length - 1 else np.nan,
                    'optimal_distance': optimal_distance,
                    'optimal_path': optimal_path,
                    'coordinates': [],
                    'state_values': state_value
                })

        return data

    def generate_full_experiment_data(self) -> pd.DataFrame:
        """ç”Ÿæˆå®Œæ•´çš„å®éªŒæ•°æ®"""
        all_data = []

        # é€‰æ‹©å®éªŒå‚æ•°
        selected_algorithms = ['DQN_LSTM', 'ActorCritic'][:1]
        selected_cities = [10, 20][:1]
        selected_instances = list(range(0, 10, 2))

        total_combinations = len(selected_algorithms) * len(selected_cities) * len(self.modes) * len(self.state_types)
        current_combination = 0

        print("å¼€å§‹ç”Ÿæˆæ¶ˆèå®éªŒæ•°æ®...")
        print(f"çŠ¶æ€ç»„åˆæ€»æ•°: {len(self.state_types)}")

        for algorithm in selected_algorithms:
            for city_num in selected_cities:
                for mode in self.modes:
                    for state_type in self.state_types:
                        current_combination += 1
                        print(
                            f"å¤„ç†ç»„åˆ {current_combination}/{total_combinations}: {algorithm}-{city_num}-{mode}-{state_type}")

                        if mode == 'per_instance':
                            # è®­ç»ƒé›†
                            for instance_id in selected_instances[:2]:
                                for run_id in range(2):
                                    episode_data = self.generate_episode_data(
                                        algorithm, city_num, mode, state_type,
                                        instance_id, run_id, 'train'
                                    )
                                    all_data.extend(episode_data[-20:])

                            # æµ‹è¯•é›†
                            for instance_id in selected_instances[2:3]:
                                for run_id in range(2):
                                    episode_data = self.generate_episode_data(
                                        algorithm, city_num, mode, state_type,
                                        instance_id, run_id, 'test'
                                    )
                                    all_data.extend(episode_data[-10:])

                        else:  # cross_instance
                            for run_id in range(2):
                                for instance_id in selected_instances[:2]:
                                    episode_data = self.generate_episode_data(
                                        algorithm, city_num, mode, state_type,
                                        instance_id, run_id, 'train'
                                    )
                                    all_data.extend(episode_data[-15:])

                                for instance_id in selected_instances[2:3]:
                                    episode_data = self.generate_episode_data(
                                        algorithm, city_num, mode, state_type,
                                        instance_id, run_id, 'test'
                                    )
                                    all_data.extend(episode_data[-8:])

        df = pd.DataFrame(all_data)
        print(f"æ•°æ®ç”Ÿæˆå®Œæˆï¼æ€»å…± {len(df)} è¡Œæ•°æ®")
        return df




try:
    # 1. æ˜¾ç¤ºçŠ¶æ€ç»„åˆæ˜ å°„
    print("=" * 80)
    print("TSPæ¶ˆèå®éªŒçŠ¶æ€ç»„åˆæ˜ å°„")
    print("=" * 80)
    for state_type, components in map_state_types.items():
        print(f"{state_type}: {components}")
    print("=" * 80)

    # 2. åˆ›å»ºå®éªŒæ•°æ®ç”Ÿæˆå™¨
    print("\næ­¥éª¤ 1: åˆå§‹åŒ–å®éªŒæ•°æ®ç”Ÿæˆå™¨...")
    generator = TSPAblationExperimentGenerator()

    # 3. ç”Ÿæˆå®éªŒæ•°æ®
    print("\næ­¥éª¤ 2: ç”Ÿæˆå®éªŒæ•°æ®...")
    df = generator.generate_full_experiment_data()

    # 3. ä¿å­˜åŸå§‹æ•°æ®
    print("\næ­¥éª¤ 3: ä¿å­˜åŸå§‹æ•°æ®...")
    df.to_csv('tsp_ablation_experiment_data.csv', index=False)

    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
    print("â”œâ”€ tsp_ablation_experiment_data.csv (åŸå§‹æ•°æ®)")


except Exception as e:
    print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

