
import traceback
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
from typing import Dict, List, Tuple, Any
import warnings
from scipy import stats
from sklearn.metrics import mean_squared_error
import os
from itertools import combinations

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


warnings.filterwarnings('ignore')

import csv
import os
from collections import defaultdict
import os
import csv
import time
from matplotlib.lines import Line2D

from collections import defaultdict


def split_large_csv(input_file, output_dir=None, buffer_size=1000000):
    """
    æ‹†åˆ†å¤§CSVæ–‡ä»¶æŒ‰(mode, train_test)åˆ†ç»„ - ä¼˜åŒ–ç‰ˆæœ¬

    Args:
        input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨è¾“å…¥æ–‡ä»¶çš„åŒçº§ç›®å½•
        buffer_size: æ¯ä¸ªæ–‡ä»¶çš„ç¼“å†²åŒºå¤§å°ï¼ˆè¡Œæ•°ï¼‰ï¼Œé»˜è®¤1000è¡Œ

    Returns:
        list: ç”Ÿæˆçš„æ‰€æœ‰å°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„åˆ—è¡¨
    """
    # å¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨è¾“å…¥æ–‡ä»¶çš„åŒçº§ç›®å½•
    if output_dir is None:
        output_dir = os.path.dirname(os.path.abspath(input_file))

    os.makedirs(output_dir, exist_ok=True)

    # ä½¿ç”¨æ›´å¤§çš„æ–‡ä»¶è¯»å–ç¼“å†²åŒº
    file_buffer_size = 8192 * 16  # 128KBç¼“å†²åŒº

    # å¿«é€Ÿä¼°ç®—æ–‡ä»¶å¤§å°å’Œè¡Œæ•°
    print("æ­£åœ¨ä¼°ç®—æ–‡ä»¶å¤§å°...")
    file_size = os.path.getsize(input_file)

    # é‡‡æ ·å‰1000è¡Œæ¥ä¼°ç®—å¹³å‡è¡Œé•¿åº¦
    sample_lines = 0
    sample_size = 0
    with open(input_file, 'r', encoding='utf-8', buffering=file_buffer_size) as f:
        next(f)  # è·³è¿‡è¡¨å¤´
        for i, line in enumerate(f):
            if i >= 1000:  # é‡‡æ ·1000è¡Œ
                break
            sample_lines += 1
            sample_size += len(line.encode('utf-8'))

    # ä¼°ç®—æ€»è¡Œæ•°
    if sample_lines > 0:
        avg_line_size = sample_size / sample_lines
        estimated_lines = int((file_size - sample_size) / avg_line_size)
        print(f"ä¼°ç®—æ–‡ä»¶çº¦æœ‰ {estimated_lines:,} è¡Œæ•°æ®")
    else:
        estimated_lines = 0

    # ä½¿ç”¨ç¼“å†²åŒºæ‰¹é‡å†™å…¥
    file_buffers = defaultdict(list)  # æ¯ä¸ªæ–‡ä»¶çš„æ•°æ®ç¼“å†²åŒº
    file_handles = {}
    writers = {}
    output_files = []
    processed_count = 0
    start_time = time.time()

    def flush_buffer(filename):
        """åˆ·æ–°æŒ‡å®šæ–‡ä»¶çš„ç¼“å†²åŒº"""
        if filename in file_buffers and file_buffers[filename]:
            for row in file_buffers[filename]:
                writers[filename].writerow(row)
            file_buffers[filename].clear()

    def flush_all_buffers():
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº"""
        for filename in list(file_buffers.keys()):
            flush_buffer(filename)

    try:
        with open(input_file, 'r', encoding='utf-8', buffering=file_buffer_size) as infile:
            reader = csv.DictReader(infile)
            fieldnames = reader.fieldnames

            for row in reader:
                mode = row['mode']
                train_test = row['train_test']

                # ç”Ÿæˆæ–‡ä»¶å
                safe_mode = str(mode).replace('/', '_').replace('\\', '_')
                safe_train_test = str(train_test).replace('/', '_').replace('\\', '_')
                filename = f"{safe_mode}_{safe_train_test}.csv"
                filepath = os.path.join(output_dir, filename)

                # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œåˆ›å»ºæ–‡ä»¶å¥æŸ„å’Œwriter
                if filename not in file_handles:
                    file_handles[filename] = open(filepath, 'w', encoding='utf-8',
                                                  newline='', buffering=file_buffer_size)
                    writers[filename] = csv.DictWriter(
                        file_handles[filename],
                        fieldnames=fieldnames
                    )
                    writers[filename].writeheader()
                    output_files.append(filepath)
                    print(f"åˆ›å»ºæ–°æ–‡ä»¶: {filename}")

                # æ·»åŠ åˆ°ç¼“å†²åŒºè€Œä¸æ˜¯ç›´æ¥å†™å…¥
                file_buffers[filename].append(row)

                # å½“ç¼“å†²åŒºè¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œæ‰¹é‡å†™å…¥
                if len(file_buffers[filename]) >= buffer_size:
                    flush_buffer(filename)

                processed_count += 1

                # æ¯å¤„ç†5000è¡Œæ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆå‡å°‘æ‰“å°é¢‘ç‡æå‡æ€§èƒ½ï¼‰
                if processed_count % 5000 == 0:
                    elapsed_time = time.time() - start_time
                    if estimated_lines > 0:
                        progress_percent = (processed_count / estimated_lines) * 100
                        speed = processed_count / elapsed_time if elapsed_time > 0 else 0

                        print(f"è¿›åº¦: {processed_count:,}/{estimated_lines:,} ({progress_percent:.1f}%) "
                              f"| é€Ÿåº¦: {speed:.0f} è¡Œ/ç§’ | æ–‡ä»¶æ•°: {len(output_files)}")
                    else:
                        speed = processed_count / elapsed_time if elapsed_time > 0 else 0
                        print(f"å·²å¤„ç†: {processed_count:,} è¡Œ | é€Ÿåº¦: {speed:.0f} è¡Œ/ç§’ | æ–‡ä»¶æ•°: {len(output_files)}")

        # å¤„ç†å®Œæˆååˆ·æ–°æ‰€æœ‰å‰©ä½™çš„ç¼“å†²åŒº
        print("æ­£åœ¨å†™å…¥å‰©ä½™æ•°æ®...")
        flush_all_buffers()

    finally:
        # ç¡®ä¿æ‰€æœ‰æ–‡ä»¶å¥æŸ„éƒ½è¢«å…³é—­
        for handle in file_handles.values():
            handle.close()

    elapsed_time = time.time() - start_time
    print(f"\næ‹†åˆ†å®Œæˆï¼")
    print(f"æ€»å…±å¤„ç†äº† {processed_count:,} è¡Œæ•°æ®")
    print(f"ç”Ÿæˆäº† {len(output_files)} ä¸ªæ–‡ä»¶")
    print(f"æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"å¹³å‡é€Ÿåº¦: {processed_count / elapsed_time:.0f} è¡Œ/ç§’")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    return output_files


# æ„å»ºçŠ¶æ€å˜é‡æ˜ å°„å…³ç³»
def build_state_combinations():
    """æ„å»ºæ¶ˆèå®éªŒçš„çŠ¶æ€ç»„åˆæ˜ å°„"""
    base_states = ["current_city_onehot", "visited_mask", "order_embedding", "distances_from_current"]

    map_state_types = {}

    # 1. å…¨çŠ¶æ€ï¼ˆåŸºçº¿ï¼‰
    map_state_types['full'] = base_states.copy()

    # 2. ä¾æ¬¡ç§»é™¤ä¸€ç§çŠ¶æ€ï¼ˆ4ç§ç»„åˆï¼‰
    for i, state_to_remove in enumerate(base_states):
        remaining_states = [s for s in base_states if s != state_to_remove]
        map_state_types[f'ablation_remove_{state_to_remove.split("_")[0]}'] = remaining_states

    # 3. ä¾æ¬¡ç§»é™¤ä¸¤ç§çŠ¶æ€ï¼ˆ6ç§ç»„åˆï¼‰
    for i, (state1, state2) in enumerate(combinations(base_states, 2)):
        remaining_states = [s for s in base_states if s not in [state1, state2]]
        key_name = f'ablation_remove_{state1.split("_")[0]}_{state2.split("_")[0]}'
        map_state_types[key_name] = remaining_states

    return map_state_types


# å…¨å±€æ˜ å°„å…³ç³»
map_state_types = build_state_combinations()
full_states = ["current_city_onehot", "visited_mask", "order_embedding", "distances_from_current"]


class TSPAdvancedAblationAnalyzer:
    """é«˜çº§TSPæ¶ˆèå®éªŒåˆ†æå™¨ - åšå£«æ°´å‡†"""

    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.results = {}

    # def calculate_performance_metrics(self) -> pd.DataFrame:
    #     """è®¡ç®—æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡"""
    #     print("è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    #     episode_data = self.df[self.df['done'] == 1].copy()
    #     episode_data['optimality_gap'] = (
    #             (episode_data['current_distance'] - episode_data['optimal_distance']) /
    #             episode_data['optimal_distance'] * 100
    #     )
    #     episode_data.to_csv('t3.csv',index=False)
    #     # ç¬¬ä¸€æ¬¡åˆ†ç»„èšåˆï¼Œå¹¶è®¾ç½®åˆ—åå‰ç¼€ä¸º "instance_"
    #     instance_metrics = episode_data.groupby(
    #         ['algorithm', 'city_num', 'mode', 'state_type', 'train_test', 'instance_id']).agg({
    #         'optimality_gap': ['min'],  # ['mean', 'std', 'max', 'min', 'count'],
    #         'total_reward': ['max']  # ['mean', 'std', 'max', 'min', 'count'],
    #     }).round(4)
    #
    #
    #     # é‡æ„åˆ—åï¼Œå°†å¤šçº§åˆ—åå±•å¹³å¹¶æ·»åŠ  "instance_" å‰ç¼€
    #     instance_metrics.columns = ['instance_' + '_'.join(col).strip() for col in instance_metrics.columns.values]
    #
    #     # é‡ç½®ç´¢å¼•ï¼Œä½¿åˆ†ç»„åˆ—å˜ä¸ºæ™®é€šåˆ—
    #     instance_metrics = instance_metrics.reset_index()
    #
    #     # ç¬¬äºŒæ¬¡åˆ†ç»„èšåˆ
    #     # é¦–å…ˆè·å–æ‰€æœ‰ä»¥ "instance_" å¼€å¤´çš„åˆ—å
    #     instance_columns = [col for col in instance_metrics.columns if col.startswith('instance_') and col != 'instance_id']
    #
    #     # æ„å»ºç¬¬äºŒæ¬¡èšåˆçš„å­—å…¸
    #     agg_dict = {}
    #     for col in instance_columns:
    #         agg_dict[col] = ['mean', 'std', 'max', 'min', 'count']
    #
    #     # è¿›è¡Œç¬¬äºŒæ¬¡åˆ†ç»„èšåˆ
    #     metrics = instance_metrics.groupby(['algorithm', 'city_num', 'mode', 'state_type', 'train_test']).agg(
    #         agg_dict).round(4)
    #     metrics = metrics.fillna(0)
    #
    #     # é‡æ„æœ€ç»ˆåˆ—å
    #     final_columns = []
    #     for col in metrics.columns.values:
    #         if col[0].startswith('instance_optimality_gap'):
    #             # ä» instance_optimality_gap_xxx æå–åé¢çš„éƒ¨åˆ†ï¼Œç„¶åä¸èšåˆå‡½æ•°ç»„åˆ
    #             original_metric = col[0].replace('instance_', '').split('_')[0] + '_' + \
    #                               col[0].replace('instance_', '').split('_')[1]  # optimality_gap
    #             final_columns.append(f"optimality_gap_{col[1]}")
    #         elif col[0].startswith('instance_total_reward'):
    #             # ä» instance_total_reward_xxx æå–åé¢çš„éƒ¨åˆ†ï¼Œç„¶åä¸èšåˆå‡½æ•°ç»„åˆ
    #             final_columns.append(f"total_reward_{col[1]}")
    #         else:
    #             # å…¶ä»–æƒ…å†µä¿æŒåŸæ ·æˆ–æ ¹æ®éœ€è¦å¤„ç†
    #             final_columns.append('_'.join(col).strip())
    #
    #     metrics.columns = final_columns
    #
    #     # é‡ç½®ç´¢å¼•
    #     metrics = metrics.reset_index()
    #
    #
    #     # metrics = metrics.compute()
    #
    #     return metrics
    def calculate_performance_metrics(self) -> pd.DataFrame:
        """è®¡ç®—æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡"""
        print("è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
        episode_data = self.df[self.df['done'] == 1].copy()
        episode_data['optimality_gap'] = (
                (episode_data['current_distance'] - episode_data['optimal_distance']) /
                episode_data['optimal_distance'] * 100
        )
        episode_data.to_csv('t3.csv',index=False)
        # ç¬¬ä¸€æ¬¡åˆ†ç»„èšåˆï¼Œå¹¶è®¾ç½®åˆ—åå‰ç¼€ä¸º "instance_"
        metrics = episode_data.groupby(
            ['algorithm', 'city_num', 'mode', 'state_type', 'train_test', 'instance_id']).agg({
            'optimality_gap': ['mean', 'std', 'max', 'min', 'count'],
            'total_reward':  ['mean', 'std', 'max', 'min', 'count'],
        }).round(4)
        metrics.columns = ['_'.join(col).strip() for col in metrics.columns.values]

        # é‡ç½®ç´¢å¼•
        metrics = metrics.reset_index()

        return metrics


    def _welch_t_test(self, mean1, std1, n1, mean2, std2, n2):
        """
        Welch's t-testå®ç° - çœŸæ­£çš„åŒæ ·æœ¬tæ£€éªŒ

        å…¬å¼ï¼š
        t = (Î¼â‚ - Î¼â‚‚) / SE_diff
        SE_diff = âˆš(sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)
        df = (sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)Â² / [(sâ‚Â²/nâ‚)Â²/(nâ‚-1) + (sâ‚‚Â²/nâ‚‚)Â²/(nâ‚‚-1)]

        å…¶ä¸­ï¼š
        - Î¼â‚, Î¼â‚‚: ä¸¤ç»„æ ·æœ¬å‡å€¼
        - sâ‚, sâ‚‚: ä¸¤ç»„æ ·æœ¬æ ‡å‡†å·®
        - nâ‚, nâ‚‚: ä¸¤ç»„æ ·æœ¬æ•°é‡
        - SE_diff: å‡å€¼å·®çš„æ ‡å‡†è¯¯å·®
        - df: è‡ªç”±åº¦
        """
        if std1 <= 0 or std2 <= 0 or n1 <= 1 or n2 <= 1:
            return 0.0, 1.0

        # æ ‡å‡†è¯¯å·®
        se1 = std1 / np.sqrt(n1)
        se2 = std2 / np.sqrt(n2)
        se_diff = np.sqrt(se1 ** 2 + se2 ** 2)

        # tç»Ÿè®¡é‡
        t_stat = (mean1 - mean2) / se_diff

        # è‡ªç”±åº¦ (Welch-Satterthwaiteæ–¹ç¨‹)
        df = (se1 ** 2 + se2 ** 2) ** 2 / (se1 ** 4 / (n1 - 1) + se2 ** 4 / (n2 - 1))

        # på€¼ (åŒå°¾æ£€éªŒ)
        p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))

        return t_stat, p_value

    def _perform_real_significance_tests(self, subset: pd.DataFrame, full_stats: Dict) -> Dict[str, float]:
        """æ‰§è¡ŒçœŸæ­£çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:ç”¨äºåˆ¤æ–­æ¶ˆèå®éªŒä¸­ç§»é™¤æŸä¸ªçŠ¶æ€ç»„ä»¶åçš„æ€§èƒ½å˜åŒ–æ˜¯å¦å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„æ˜¾è‘—æ€§ã€‚
        å¯¹æ¯ä¸ªæ¶ˆèçŠ¶æ€ï¼ˆç§»é™¤ç»„ä»¶åçš„çŠ¶æ€ï¼‰ä¸åŸºçº¿çŠ¶æ€ï¼ˆfullçŠ¶æ€ï¼‰è¿›è¡ŒåŒæ ·æœ¬tæ£€éªŒ
        è®¡ç®—æ•ˆåº”é‡ï¼ˆCohen's dï¼‰æ¥è¡¡é‡å®é™…å·®å¼‚çš„å¤§å°
        åˆ¤æ–­æ€§èƒ½å·®å¼‚æ˜¯å¦æ˜¾è‘—ï¼ˆp < 0.05ï¼‰

        ---->ç¡®å®šè§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®å¼‚æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œè€Œä¸æ˜¯ç”±äºéšæœºè¯¯å·®é€ æˆçš„ã€‚

        """
        significance = {}

        for _, row in subset.iterrows():
            if row['state_type'] != 'full':
                # åŒæ ·æœ¬Welch's tæ£€éªŒ: fullå¯¹æ¯” å„ä¸ªæ¶ˆèå®éªŒ
                # Welch's t-testï¼ˆåŒæ ·æœ¬tæ£€éªŒï¼‰
                t_stat, p_value = self._welch_t_test(
                    full_stats['mean'], full_stats['std'], full_stats['count'],
                    row['optimality_gap_mean'], row['optimality_gap_std'], row['optimality_gap_count']
                )

                # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
                # åˆå¹¶æ ‡å‡†å·®å…¬å¼ï¼špooled_std = âˆš[((nâ‚-1)Ã—sâ‚Â² + (nâ‚‚-1)Ã—sâ‚‚Â²) / (nâ‚+nâ‚‚-2)]
                pooled_std = np.sqrt(((full_stats['count'] - 1) * full_stats['std'] ** 2 +
                                      (row['optimality_gap_count'] - 1) * row['optimality_gap_std'] ** 2) /
                                     (full_stats['count'] + row['optimality_gap_count'] - 2))

                # Cohen's då…¬å¼ï¼šCohen's_d = |Î¼â‚ - Î¼â‚‚| / pooled_std
                cohens_d = abs(row['optimality_gap_mean'] - full_stats['mean']) / pooled_std if pooled_std > 0 else 0

                significance[f"{row['state_type']}_p_value"] = p_value
                significance[f"{row['state_type']}_t_statistic"] = t_stat

                # ä½¿ç”¨åŒå°¾æ£€éªŒï¼Œå› ä¸ºæˆ‘ä»¬å…³å¿ƒçš„æ˜¯å·®å¼‚çš„å­˜åœ¨æ€§ï¼Œä¸é¢„è®¾æ–¹å‘ã€‚ æ˜¾è‘—æ€§åˆ¤æ–­: p < 0.05 --->è¿™ä¸ªå·®å¼‚æ˜¯å¶ç„¶å‘ç”Ÿçš„å—ï¼Ÿ"
                # p < 0.05ï¼šå·®å¼‚å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œä¸å¤ªå¯èƒ½æ˜¯éšæœºäº§ç”Ÿçš„
                significance[f"{row['state_type']}_is_significant"] = 1.0 if p_value < 0.05 else 0.0

                # æ•ˆåº”å¤§å°: Cohen's d è¡¨ç¤ºå·®å¼‚çš„å®é™…é‡è¦æ€§: é¿å…å°†éšæœºæ³¢åŠ¨è¯¯è®¤ä¸ºçœŸå®æ•ˆåº”
                # "è¿™ä¸ªå·®å¼‚æœ‰å¤šé‡è¦"
                # åªçœ‹på€¼ï¼šå¯èƒ½ä¿ç•™å¾ˆå¤š"ç»Ÿè®¡æ˜¾è‘—ä½†æ— å®é™…æ„ä¹‰"çš„ç»„ä»¶
                # åªçœ‹Cohen's dï¼šå¯èƒ½è¢«éšæœºæ³¢åŠ¨è¯¯å¯¼
                # ä¸¤è€…ç»“åˆï¼šæ—¢ä¿è¯ç§‘å­¦ä¸¥è°¨æ€§ï¼Œåˆå…³æ³¨å®é™…æ„ä¹‰
                significance[f"{row['state_type']}_effect_size"] = cohens_d
        # | på€¼èŒƒå›´  | Cohen's dèŒƒå›´ | ç»„ä»¶é‡è¦æ€§ | å»ºè®®è¡ŒåŠ¨ | ç½®ä¿¡åº¦ |
        # | <0.01   | >0.8        | æå…¶é‡è¦    | å¿…é¡»ä¿ç•™ | å¾ˆé«˜  |
        # | <0.05   | 0.5-0.8     | é‡è¦       | å»ºè®®ä¿ç•™ | é«˜   |
        # | <0.05   | 0.2-0.5     | ä¸€èˆ¬é‡è¦    | å¯ä»¥ä¿ç•™ | ä¸­ç­‰  |
        # | <0.05   | <0.2        | æ¬¡è¦       | å¯ä»¥ç§»é™¤ | ä¸­ç­‰  |
        # | â‰¥0.05   | >0.8        | ä¸ç¡®å®š      | å¢åŠ æ ·æœ¬ | ä½   |
        # | â‰¥0.05   | <0.5        | ä¸é‡è¦      | å¯ä»¥ç§»é™¤ | é«˜   |

        return significance

    def calculate_component_contributions(self,metrics) -> pd.DataFrame:
        """é«˜çº§ç»„ä»¶è´¡çŒ®åº¦åˆ†æ - åŸºäºæ¶ˆèå®éªŒç†è®º"""
        print("æ‰§è¡Œé«˜çº§ç»„ä»¶è´¡çŒ®åº¦åˆ†æ...")
        if  metrics is not None:
            metrics = self.calculate_performance_metrics()
        contributions = []

        for algorithm in metrics['algorithm'].unique():
            for city_num in metrics['city_num'].unique():
                for mode in metrics['mode'].unique():
                    for train_test in metrics['train_test'].unique():

                        subset = metrics[
                            (metrics['algorithm'] == algorithm) &
                            (metrics['city_num'] == city_num) &
                            (metrics['mode'] == mode) &
                            (metrics['train_test'] == train_test)
                            ]

                        if len(subset) == 0:
                            continue

                        # è·å–å…¨çŠ¶æ€çš„ç»Ÿè®¡ä¿¡æ¯
                        full_row = subset[subset['state_type'] == 'full']
                        if len(full_row) == 0:
                            continue

                        full_stats = {
                            'mean': full_row.iloc[0]['optimality_gap_mean'],
                            'std': full_row.iloc[0]['optimality_gap_std'],
                            'count': full_row.iloc[0]['optimality_gap_count']
                        }

                        # æ„å»ºæ€§èƒ½å­—å…¸
                        performance_dict = {}
                        for _, row in subset.iterrows():
                            state_type = row['state_type']
                            performance_dict[state_type] = row['optimality_gap_mean']

                        if 'full' not in performance_dict:
                            continue

                        full_performance = performance_dict['full']

                        # è®¡ç®—å„ç»„ä»¶çš„è¾¹é™…è´¡çŒ®
                        component_contributions = self._calculate_marginal_contributions(performance_dict)

                        # è®¡ç®—äº¤äº’æ•ˆåº”:  è¯¥å‡½æ•°è®¡ç®—å½“åŒæ—¶ç§»é™¤ä¸¤ä¸ªç»„ä»¶æ—¶ï¼Œäº§ç”Ÿçš„äº¤äº’æ•ˆåº”æ˜¯å¦å¤§äºã€å°äºæˆ–ç­‰äºå•ç‹¬ç§»é™¤è¿™ä¸¤ä¸ªç»„ä»¶æ•ˆåº”çš„ç®€å•ç›¸åŠ ã€‚
                        # å“ªäº›çŠ¶æ€ç»„ä»¶å¿…é¡»åŒæ—¶ä¿ç•™ï¼ˆæ­£äº¤äº’æ•ˆåº”ï¼‰
                        # å“ªäº›ç»„ä»¶å¯èƒ½åŠŸèƒ½é‡å¤ï¼ˆè´Ÿäº¤äº’æ•ˆåº”ï¼‰
                        # å“ªäº›ç»„ä»¶ç›¸äº’ç‹¬ç«‹ï¼ˆé›¶äº¤äº’æ•ˆåº”ï¼‰
                        interaction_effects = self._calculate_interaction_effects(performance_dict)

                        # è®¡ç®—å„ç»„ä»¶çš„è¾¹é™…è´¡çŒ®æ’åº: é‡è¦æ€§æ’åº
                        importance_ranking = self._calculate_importance_ranking(performance_dict)

                        # çœŸæ­£çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ   fullå¯¹æ¯” å„ä¸ªæ¶ˆèå®éªŒ
                        # ç”¨äºåˆ¤æ–­æ¶ˆèå®éªŒä¸­ç§»é™¤æŸä¸ªçŠ¶æ€ç»„ä»¶åçš„æ€§èƒ½å˜åŒ–æ˜¯å¦å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„æ˜¾è‘—æ€§ã€‚ t-testçš„p tå€¼ï¼ŒCohen's d
                        # --->ç¡®å®šè§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®å¼‚æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œè€Œä¸æ˜¯ç”±äºéšæœºè¯¯å·®é€ æˆçš„ã€‚
                        significance_tests = self._perform_real_significance_tests(subset, full_stats)

                        result = {
                            'algorithm': algorithm,
                            'city_num': city_num,
                            'mode': mode,
                            'train_test': train_test,
                            'full_performance': full_performance,
                            **component_contributions,
                            **interaction_effects,
                            **importance_ranking,
                            **significance_tests
                        }

                        contributions.append(result)

        return pd.DataFrame(contributions)

    def _calculate_marginal_contributions(self, performance_dict: Dict[str, float]) -> Dict[str, float]:
        """
        è®¡ç®—å„ç»„ä»¶çš„è¾¹é™…è´¡çŒ®

        è¾¹é™…è´¡çŒ®å…¬å¼ï¼š
        MC_i = P(S \ {i}) - P(S)

        å…¶ä¸­ï¼š
        - MC_i: ç»„ä»¶içš„è¾¹é™…è´¡çŒ®
        - P(S): å®Œæ•´çŠ¶æ€é›†åˆSçš„æ€§èƒ½
        - P(S \ {i}): ç§»é™¤ç»„ä»¶iåçš„æ€§èƒ½
        - è¾¹é™…è´¡çŒ®ä¸ºæ­£å€¼è¡¨ç¤ºç»„ä»¶é‡è¦ï¼ˆç§»é™¤åæ€§èƒ½ä¸‹é™ï¼‰

        ä¾‹å­ï¼š
        å‡è®¾å®Œæ•´çŠ¶æ€æ€§èƒ½ä¸º15%ä¼˜åŒ–å·®è·ï¼Œç§»é™¤visited_maskåä¸º25%
        åˆ™visited_maskçš„è¾¹é™…è´¡çŒ® = 25% - 15% = 10%ï¼ˆé‡è¦ç»„ä»¶ï¼‰

        å‡è®¾ç§»é™¤distances_from_currentåä¸º16%
        åˆ™distances_from_currentçš„è¾¹é™…è´¡çŒ® = 16% - 15% = 1%ï¼ˆæ¬¡è¦ç»„ä»¶ï¼‰
        """
        contributions = {}
        full_perf = performance_dict.get('full', 0)

        # è®¡ç®—å•ç»„ä»¶ç§»é™¤çš„å½±å“
        for component in full_states:
            remove_key = f'ablation_remove_{component.split("_")[0]}'
            if remove_key in performance_dict:
                # è¾¹é™…è´¡çŒ® = ç§»é™¤è¯¥ç»„ä»¶åçš„æ€§èƒ½ä¸‹é™
                contribution = performance_dict[remove_key] - full_perf
                contributions[f'{component}_marginal_contribution'] = contribution
            else:
                contributions[f'{component}_marginal_contribution'] = 0.0

        return contributions

    def _calculate_interaction_effects(self, performance_dict: Dict[str, float]) -> Dict[str, float]:
        """
        è®¡ç®—ç»„ä»¶é—´çš„äº¤äº’æ•ˆåº”

        ç®€åŒ–ä¸ºï¼š
        IE_{i,j} = P(S \ {i,j}) - ( P(S \ {i}) + P(S \ {j}) )
        å…¶ä¸­ï¼š
        - IE_{i,j}: ç»„ä»¶iå’Œjçš„äº¤äº’æ•ˆåº”
        - P(S \ {i,j}): åŒæ—¶ç§»é™¤ç»„ä»¶iå’Œjåçš„æ€§èƒ½
        - P(S \ {i}): åªç§»é™¤ç»„ä»¶iåçš„æ€§èƒ½
        - P(S \ {j}): åªç§»é™¤ç»„ä»¶jåçš„æ€§èƒ½

        äº¤äº’æ•ˆåº”è§£é‡Šï¼š
        - æ­£å€¼ï¼šååŒæ•ˆåº”ï¼ˆä¸¤ç»„ä»¶é…åˆä½¿ç”¨æ•ˆæœæ›´å¥½ï¼‰
        - è´Ÿå€¼ï¼šå†—ä½™æ•ˆåº”ï¼ˆä¸¤ç»„ä»¶åŠŸèƒ½é‡å ï¼‰
        - é›¶å€¼ï¼šç‹¬ç«‹æ•ˆåº”ï¼ˆä¸¤ç»„ä»¶æ— äº¤äº’ï¼‰
        """
        interactions = {}
        full_perf = performance_dict.get('full', 0)

        # è®¡ç®—ä¸¤ä¸¤ç»„ä»¶çš„äº¤äº’æ•ˆåº”
        for i, comp1 in enumerate(full_states):
            for j, comp2 in enumerate(full_states[i + 1:], i + 1):
                comp1_short = comp1.split("_")[0]
                comp2_short = comp2.split("_")[0]

                # æ„å»ºåŒæ—¶ç§»é™¤ä¸¤ä¸ªç»„ä»¶çš„çŠ¶æ€é”®
                remove_both_key = f'ablation_remove_{comp1_short}_{comp2_short}'

                if remove_both_key in performance_dict:
                    remove_comp1_key = f'ablation_remove_{comp1_short}'
                    remove_comp2_key = f'ablation_remove_{comp2_short}'

                    if remove_comp1_key in performance_dict and remove_comp2_key in performance_dict:
                        # äº¤äº’æ•ˆåº” = åŒæ—¶ç§»é™¤çš„å½±å“ - å•ç‹¬ç§»é™¤çš„å½±å“ä¹‹å’Œ
                        both_removed = performance_dict[remove_both_key] - full_perf
                        comp1_removed = performance_dict[remove_comp1_key] - full_perf
                        comp2_removed = performance_dict[remove_comp2_key] - full_perf

                        interaction = both_removed - (comp1_removed + comp2_removed)
                        interactions[f'{comp1_short}_{comp2_short}_interaction'] = interaction

        return interactions

    def _calculate_importance_ranking(self, performance_dict: Dict[str, float]) -> Dict[str, any]:
        """
        è®¡ç®—ç»„ä»¶é‡è¦æ€§æ’åº

        é‡è¦æ€§åº¦é‡ï¼šåŸºäºè¾¹é™…è´¡çŒ®çš„ç»å¯¹å€¼
        Importance_i = |MC_i| = |P(S \ {i}) - P(S)|

        æ’åºè§„åˆ™ï¼š
        1. è¾¹é™…è´¡çŒ®ç»å¯¹å€¼è¶Šå¤§ï¼Œé‡è¦æ€§è¶Šé«˜
        2. åŒæ—¶è€ƒè™‘ç»Ÿè®¡æ˜¾è‘—æ€§
        3. æä¾›é‡è¦æ€§ç­‰çº§åˆ†ç±»

        ä¾‹å­ï¼š
        visited_mask: MC = 10% â†’ é‡è¦æ€§rank = 1 (æœ€é‡è¦)
        current_city: MC = 8% â†’ é‡è¦æ€§rank = 2
        order_embedding: MC = 3% â†’ é‡è¦æ€§rank = 3
        distances: MC = 1% â†’ é‡è¦æ€§rank = 4 (æœ€ä¸é‡è¦)
        """
        full_perf = performance_dict.get('full', 0)
        component_impacts = {}

        for component in full_states:
            remove_key = f'ablation_remove_{component.split("_")[0]}'
            if remove_key in performance_dict:
                impact = performance_dict[remove_key] - full_perf
                component_impacts[component] = abs(impact)

        # æŒ‰å½±å“ç¨‹åº¦æ’åºï¼ˆå½±å“è¶Šå¤§è¶Šé‡è¦ï¼‰
        sorted_components = sorted(component_impacts.items(), key=lambda x: abs(x[1]), reverse=True)

        ranking = {}
        for rank, (component, impact) in enumerate(sorted_components, 1):
            ranking[f'{component}_importance_rank'] = rank
            ranking[f'{component}_impact_magnitude'] = impact

        return ranking

    def _analyze_ablation_pathways(self, performance_dict: Dict[str, float],
                                   performance_better_when: str = 'smaller') -> Dict[str, Dict]:
        """
        å¯¹æ¯”  å‰”é™¤ çŠ¶æ€ä¸ªæ•° çš„è¡°å‡ï¼ˆç»„åˆä¸­æœ€å°è¡°å‡ å’Œ æœ€å¤§è¡°å‡ï¼‰

        Args:
            performance_dict: æ€§èƒ½å­—å…¸
            performance_better_when: 'smaller'è¡¨ç¤ºè¶Šå°è¶Šå¥½ï¼Œ'larger'è¡¨ç¤ºè¶Šå¤§è¶Šå¥½
        """
        pathways = {}
        full_perf = performance_dict.get('full', 0)

        # æ ¹æ®ä¼˜åŒ–æ–¹å‘ç¡®å®š"æ›´å¥½"çš„å«ä¹‰
        if performance_better_when == 'smaller':
            # å¯¹äºoptimality_gapç­‰æŒ‡æ ‡ï¼Œè¶Šå°è¶Šå¥½
            def is_better(val1, val2):
                return val1 < val2

            def get_degradation(current, baseline):
                return current - baseline  # æ­£å€¼è¡¨ç¤ºæ€§èƒ½å˜å·®
        else:
            # å¯¹äºrewardç­‰æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½
            def is_better(val1, val2):
                return val1 > val2

            def get_degradation(current, baseline):
                return baseline - current  # æ­£å€¼è¡¨ç¤ºæ€§èƒ½å˜å·®

        # æ„å»ºåŸºäºå®é™…æ•°æ®çš„è·¯å¾„
        available_pathways = {}

        for state_key, performance in performance_dict.items():
            if state_key.startswith('ablation_remove_') and state_key != 'full':
                removed_components = state_key.replace('ablation_remove_', '').split('_')
                print(f"removed_components={removed_components}")
                num_removed = len(removed_components)

                if num_removed not in available_pathways:
                    available_pathways[num_removed] = []

                degradation = get_degradation(performance, full_perf)
                available_pathways[num_removed].append({
                    'components': removed_components,
                    'performance': performance,
                    'degradation': degradation
                })

        # æ„å»ºæœ€ä¼˜è·¯å¾„ï¼ˆæœ€å°æ€§èƒ½é€€åŒ–ï¼‰
        actual_pathway_perf = [full_perf]
        actual_pathway_components = []


        worst_pathway_perf = [full_perf]
        worst_pathway_components =[]
        for num_removed in sorted(available_pathways.keys()): # removeçŠ¶æ€ï¼Œä»å°‘åˆ°å¤šï¼Œæ€§èƒ½é€æ¸é€€åŒ–
            # é€‰æ‹©æ€§èƒ½é€€åŒ–æœ€å°çš„ç»„åˆ
            best_combination = min(available_pathways[num_removed],
                                   key=lambda x: x['degradation'])
            actual_pathway_perf.append(best_combination['performance'])
            actual_pathway_components.append(best_combination['components'])

        # æ„å»ºæœ€åè·¯å¾„ï¼ˆæœ€å¤§æ€§èƒ½é€€åŒ–ï¼‰
            worst_combination = max(available_pathways[num_removed],
                                    key=lambda x: x['degradation'])
            worst_pathway_perf.append(worst_combination['performance'])
            worst_pathway_components.append(worst_combination['components'])


        # æŒ‰ç…§remove stateç»„åˆä¸ªæ•°ä¸ºå•ä½ç»Ÿè®¡
        pathways['optimal_actual'] = {
            'pathway_performance': actual_pathway_perf,
            'total_degradation': get_degradation(actual_pathway_perf[-1], actual_pathway_perf[0]) if len(
                actual_pathway_perf) > 1 else 0,
            'degradation_rate': [get_degradation( actual_pathway_perf[i],actual_pathway_perf[i + 1])
                                 for i in range(len(actual_pathway_perf) - 1)],
            'pathway_components': actual_pathway_components,
            'pathway_description': f'Optimal path (minimal degradation, {performance_better_when} is better)'
        }

        # æŒ‰ç…§remove stateç»„åˆä¸ªæ•°ä¸ºå•ä½ç»Ÿè®¡
        pathways['worst_case'] = {
            'pathway_performance': worst_pathway_perf,
            'total_degradation': get_degradation(worst_pathway_perf[-1], worst_pathway_perf[0]) if len(
                worst_pathway_perf) > 1 else 0,
            'degradation_rate': [get_degradation( worst_pathway_perf[i],worst_pathway_perf[i + 1])
                                 for i in range(len(worst_pathway_perf) - 1)],
            'pathway_components': worst_pathway_components,
            'pathway_description': f'Worst case path (maximal degradation, {performance_better_when} is better)'
        }

        return pathways

    def _find_closest_state(self, target_key: str, available_keys: List[str]) -> str:
        """å¯»æ‰¾æœ€æ¥è¿‘çš„çŠ¶æ€é”®"""
        target_components = set(target_key.split('_')[2:])  # ç§»é™¤ 'ablation_remove_' å‰ç¼€

        best_match = None
        best_score = -1

        for key in available_keys:
            if key.startswith('ablation_remove_'):
                key_components = set(key.split('_')[2:])

                # è®¡ç®—äº¤é›†å¤§å°ä½œä¸ºåŒ¹é…åˆ†æ•°
                intersection = len(target_components.intersection(key_components))
                if intersection > best_score:
                    best_score = intersection
                    best_match = key

        return best_match

    def calculate_ablation_pathway_analysis(self, performance_better_when='smaller', metrics=None) -> pd.DataFrame:
        """
        æ¶ˆèè·¯å¾„åˆ†æ
        Args:
            performance_better_when (str):
                - 'smaller': æ€§èƒ½æŒ‡æ ‡è¶Šå°è¶Šå¥½ï¼ˆå¦‚TSPçš„optimality_gap, distanceï¼‰
                - 'larger': æ€§èƒ½æŒ‡æ ‡è¶Šå¤§è¶Šå¥½ï¼ˆå¦‚reward, accuracyï¼‰
            metrics (pd.DataFrame, optional): é¢„è®¡ç®—çš„æ€§èƒ½æŒ‡æ ‡æ•°æ®ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨self.dfè®¡ç®—
        """
        print(f"æ‰§è¡Œæ¶ˆèè·¯å¾„åˆ†æ... (æ€§èƒ½æŒ‡æ ‡: {performance_better_when} is better)")

        if metrics is None:
            return None
        
        pathway_analysis = []

        for algorithm in metrics['algorithm'].unique():
            for city_num in metrics['city_num'].unique():
                for mode in metrics['mode'].unique():
                    for train_test in metrics['train_test'].unique():

                        subset = metrics[
                            (metrics['algorithm'] == algorithm) &
                            (metrics['city_num'] == city_num) &
                            (metrics['mode'] == mode) &
                            (metrics['train_test'] == train_test)
                            ]

                        if len(subset) == 0:
                            continue

                        performance_dict = {}
                        for _, row in subset.iterrows():
                            performance_dict[row['state_type']] = row['optimality_gap_mean'] #æ³¨è§†ï¼šè¿™é‡Œé…ç½®performanceå–å€¼

                        if 'full' not in performance_dict:
                            continue


                        # å¯¹æ¯”  å‰”é™¤ çŠ¶æ€ä¸ªæ•° çš„è¡°å‡ï¼ˆç»„åˆä¸­æœ€å°è¡°å‡ å’Œ æœ€å¤§è¡°å‡ï¼‰
                        pathways = self._analyze_ablation_pathways(performance_dict, performance_better_when)

                        # å¤„ç†æ–°çš„è·¯å¾„ç»“æ„
                        for pathway_name, pathway_data in pathways.items():

                            # å¤„ç†å…·ä½“è·¯å¾„æ•°æ®
                            pathway_performance = pathway_data.get('pathway_performance', []) # é¦–å…ƒç´ ä¸º fullçŠ¶æ€çš„performance
                            degradation_rates = pathway_data.get('degradation_rate', [])
                            total_degradation = pathway_data.get('total_degradation', 0)

                            # è®¡ç®—è·¯å¾„ç‰¹å¾æŒ‡æ ‡
                            pathway_length = len(pathway_performance)
                            max_degradation = max(degradation_rates) if degradation_rates else 0
                            min_degradation = min(degradation_rates) if degradation_rates else 0
                            degradation_variance = np.var(degradation_rates) if degradation_rates else 0

                            # è·¯å¾„æ•ˆç‡ï¼šæ€»é€€åŒ–/è·¯å¾„é•¿åº¦
                            pathway_efficiency = abs(total_degradation) / max(pathway_length - 1,
                                                                              1) if pathway_length > 1 else 0

                            result = {
                                'algorithm': algorithm,
                                'city_num': city_num,
                                'mode': mode,
                                'train_test': train_test,
                                'pathway_name': pathway_name,
                                'pathway_type': 'ablation_sequence',
                                'pathway_length': pathway_length,
                                'total_degradation': total_degradation,
                                'max_single_step_degradation': max_degradation,
                                'min_single_step_degradation': min_degradation,
                                'degradation_variance': degradation_variance,
                                'pathway_efficiency': pathway_efficiency,
                                'pathway_performance_list': str(pathway_performance),  # è½¬ä¸ºå­—ç¬¦ä¸²å­˜å‚¨
                                'degradation_rate_list': str(degradation_rates),
                                'pathway_description': pathway_data.get('pathway_description', ''),
                                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯çš„é»˜è®¤å€¼
                                'num_available_combinations': 0,
                                'max_components_removed': 0,
                                'average_single_step_degradation': np.mean(
                                    degradation_rates) if degradation_rates else 0
                            }

                            # å¦‚æœæœ‰ç»„ä»¶ä¿¡æ¯ï¼Œæ·»åŠ ç»„ä»¶åˆ†æ
                            if 'pathway_components' in pathway_data:
                                components_info = pathway_data['pathway_components']
                                result['pathway_components'] = str(components_info)

                                # åˆ†æç»„ä»¶ç§»é™¤æ¨¡å¼
                                if components_info:
                                    # è®¡ç®—æ¯æ­¥æ–°å¢ç§»é™¤çš„ç»„ä»¶æ•°
                                    step_removals = []
                                    prev_count = 0
                                    for step_components in components_info:
                                        current_count = len(step_components)
                                        step_removals.append(current_count - prev_count)
                                        prev_count = current_count

                                    result['removal_pattern'] = str(step_removals)
                                    result['final_components_removed'] = len(
                                        components_info[-1]) if components_info else 0
                            else:
                                result['pathway_components'] = ''
                                result['removal_pattern'] = ''
                                result['final_components_removed'] = 0

                            pathway_analysis.append(result)

        return pd.DataFrame(pathway_analysis)


class TSPAdvancedVisualizationSuite:
    """é«˜çº§TSPå¯è§†åŒ–å¥—ä»¶ - åšå£«æ°´å‡†"""

    def __init__(self, contributions,performance_metrics):

        self.contributions = contributions
        self.performance_metrics = performance_metrics

        # è®¾ç½®å­¦æœ¯å›¾è¡¨æ ·å¼ - ä¿®æ­£äº†æ ·å¼åç§°
        sns.set_style("whitegrid")
        sns.set_palette("viridis")

    def _get_dynamic_colors(self, n_colors, color_type='qualitative'):
        """
        åŠ¨æ€è·å–æ— é‡å¤ä¸”æœ‰æ˜æ˜¾åŒºåˆ†åº¦çš„é¢œè‰²

        Args:
            n_colors: éœ€è¦çš„é¢œè‰²æ•°é‡
            color_type: é¢œè‰²ç±»å‹ ('qualitative', 'sequential', 'diverging')

        Returns:
            list: é¢œè‰²åˆ—è¡¨
        """
        if n_colors == 0:
            return []

        if color_type == 'qualitative':
            # é«˜å¯¹æ¯”åº¦é¢œè‰²åˆ—è¡¨ï¼Œç¡®ä¿åŒºåˆ†åº¦
            high_contrast_colors = [
                '#FF0000',  # çº¢è‰²
                '#0000FF',  # è“è‰²
                '#00FF00',  # ç»¿è‰²
                '#FF8C00',  # æ·±æ©™è‰²
                '#800080',  # ç´«è‰²
                '#FF1493',  # æ·±ç²‰è‰²
                '#00CED1',  # æ·±é’è‰²
                '#FFD700',  # é‡‘è‰²
                '#8B4513',  # æ£•è‰²
                '#2E8B57',  # æµ·ç»¿è‰²
                '#4169E1',  # çš‡å®¶è“
                '#DC143C',  # æ·±çº¢è‰²
                '#32CD32',  # é…¸æ©™ç»¿
                '#FF4500',  # æ©™çº¢è‰²
                '#9932CC',  # æ·±å…°èŠ±ç´«
                '#00FFFF',  # é’è‰²
                '#FF69B4',  # çƒ­ç²‰è‰²
                '#8FBC8F',  # æ·±æµ·ç»¿
                '#B22222',  # ç«ç –çº¢
                '#00FF7F'   # æ˜¥ç»¿è‰²
            ]

            if n_colors <= len(high_contrast_colors):
                return high_contrast_colors[:n_colors]

            # å¦‚æœéœ€è¦æ›´å¤šé¢œè‰²ï¼Œä½¿ç”¨HSVè‰²å½©ç©ºé—´ç”Ÿæˆ
            colors = high_contrast_colors.copy()
            remaining = n_colors - len(colors)

            for i in range(remaining):
                hue = (i * 137.508) % 360  # é»„é‡‘è§’åº¦ï¼Œç¡®ä¿é¢œè‰²åˆ†æ•£
                saturation = 0.8 + 0.2 * (i % 2)  # åœ¨0.8-1.0ä¹‹é—´äº¤æ›¿
                value = 0.7 + 0.3 * ((i // 2) % 2)  # åœ¨0.7-1.0ä¹‹é—´äº¤æ›¿

                # HSVè½¬RGB
                import colorsys
                rgb = colorsys.hsv_to_rgb(hue/360, saturation, value)
                hex_color = '#{:02x}{:02x}{:02x}'.format(
                    int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))
                colors.append(hex_color)

            return colors[:n_colors]

        elif color_type == 'sequential':
            # é¡ºåºé¢œè‰²ï¼Œé€‚ç”¨äºæ•°å€¼æ¸å˜
            return sns.color_palette("viridis", n_colors)

        elif color_type == 'diverging':
            # å‘æ•£é¢œè‰²ï¼Œé€‚ç”¨äºæ­£è´Ÿå€¼å¯¹æ¯”
            return sns.color_palette("RdBu_r", n_colors)

        else:
            # é»˜è®¤è¿”å›qualitative
            return self._get_dynamic_colors(n_colors, 'qualitative')

    def plot_component_contribution_radar(self):
        """ç»˜åˆ¶åŸºäºçœŸå®æ•°æ®çš„ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾"""
        print("ç»˜åˆ¶ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾...")

        try:
            if len(self.contributions) == 0:
                print("No contribution data available for radar chart")
                return

            # ä»çœŸå®æ•°æ®ä¸­æå–ç»„ä»¶è´¡çŒ®ä¿¡æ¯
            marginal_cols = [col for col in self.contributions.columns if 'marginal_contribution' in col]
            if not marginal_cols:
                print("No marginal contribution data available for radar chart")
                return

            # æå–ç»„ä»¶åç§°
            component_names = []
            for col in marginal_cols:
                component = col.replace('_marginal_contribution', '').replace('_', ' ').title()
                component_names.append(component)

            # æŒ‰ç®—æ³•åˆ†ç»„è·å–è´¡çŒ®åº¦æ•°æ®
            algorithms = self.contributions['algorithm'].unique()

            if len(algorithms) == 0:
                print("No algorithm data available for radar chart")
                return

            # åŠ¨æ€ç¡®å®šè¦æ˜¾ç¤ºçš„ç®—æ³•æ•°é‡ï¼ˆæœ€å¤šæ˜¾ç¤º4ä¸ªï¼‰
            display_algorithms = algorithms[:min(4, len(algorithms))]
            n_algorithms = len(display_algorithms)

            # åˆ›å»ºå­å›¾å¸ƒå±€
            if n_algorithms == 1:
                fig, axes = plt.subplots(1, 1, figsize=(10, 10), subplot_kw={'projection': 'polar'})
                axes = [axes]
            elif n_algorithms == 2:
                fig, axes = plt.subplots(1, 2, figsize=(20, 10), subplot_kw={'projection': 'polar'})
            elif n_algorithms <= 4:
                fig, axes = plt.subplots(2, 2, figsize=(20, 20), subplot_kw={'projection': 'polar'})
                axes = axes.flatten()
            else:
                fig, axes = plt.subplots(2, 3, figsize=(24, 16), subplot_kw={'projection': 'polar'})
                axes = axes.flatten()

            # è®¾ç½®è§’åº¦
            angles = np.linspace(0, 2 * np.pi, len(component_names), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆå›¾å½¢

            # é¢œè‰²æ–¹æ¡ˆ
            colors = plt.cm.Set3(np.linspace(0, 1, n_algorithms))

            for i, algorithm in enumerate(display_algorithms):
                if i >= len(axes):
                    break

                ax = axes[i]

                # è·å–è¯¥ç®—æ³•çš„è´¡çŒ®åº¦æ•°æ®
                algo_data = self.contributions[self.contributions['algorithm'] == algorithm]

                if len(algo_data) == 0:
                    ax.text(0.5, 0.5, f'No data for {algorithm}',
                            ha='center', va='center', transform=ax.transAxes)
                    ax.set_title(f'{algorithm} - No Data', size=14, fontweight='bold')
                    continue

                # è®¡ç®—å„ç»„ä»¶çš„å¹³å‡è´¡çŒ®åº¦
                component_values = []
                for col in marginal_cols:
                    if col in algo_data.columns:
                        # ä½¿ç”¨ç»å¯¹å€¼å¹¶å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                        value = abs(algo_data[col].mean())
                        component_values.append(value)
                    else:
                        component_values.append(0.0)

                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                if max(component_values) > 0:
                    max_val = max(component_values)
                    normalized_values = [v / max_val for v in component_values]
                else:
                    normalized_values = component_values

                # é—­åˆé›·è¾¾å›¾
                radar_values = normalized_values + normalized_values[:1]

                # ç»˜åˆ¶é›·è¾¾å›¾
                ax.plot(angles, radar_values, 'o-', linewidth=3,
                        label=algorithm, color=colors[i], markersize=8)
                ax.fill(angles, radar_values, alpha=0.25, color=colors[i])

                # è®¾ç½®æ ‡ç­¾
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels(component_names, fontsize=10)

                # è®¾ç½®å¾„å‘æ ‡ç­¾
                ax.set_ylim(0, 1)
                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=8)
                ax.grid(True)

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for angle, value, name in zip(angles[:-1], normalized_values, component_names):
                    ax.text(angle, value + 0.05, f'{value:.2f}',
                            ha='center', va='center', fontsize=8, fontweight='bold')

                # è®¾ç½®æ ‡é¢˜ï¼ŒåŒ…å«å®é™…çš„è´¡çŒ®åº¦ç»Ÿè®¡ä¿¡æ¯
                mean_contribution = np.mean(component_values)
                max_contribution = max(component_values)
                ax.set_title(f'{algorithm}\nMean: {mean_contribution:.3f}, Max: {max_contribution:.3f}',
                             size=12, fontweight='bold', pad=20)

            # éšè—å¤šä½™çš„å­å›¾
            for j in range(n_algorithms, len(axes)):
                axes[j].set_visible(False)

            # æ·»åŠ æ€»ä½“å›¾ä¾‹å’Œç»Ÿè®¡ä¿¡æ¯
            if n_algorithms > 1:
                # åœ¨å›¾å¤–æ·»åŠ æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
                fig.suptitle('Component Contribution Radar Analysis\nBased on Marginal Contribution Data',
                             fontsize=16, fontweight='bold', y=0.95)

                # è®¡ç®—è·¨ç®—æ³•çš„ç»„ä»¶é‡è¦æ€§æ’åº
                overall_importance = {}
                for i, col in enumerate(marginal_cols):
                    component = component_names[i]
                    overall_value = abs(self.contributions[col].mean())
                    overall_importance[component] = overall_value

                # æ’åºå¹¶æ·»åŠ æ–‡æœ¬è¯´æ˜
                sorted_importance = sorted(overall_importance.items(), key=lambda x: x[1], reverse=True)
                importance_text = "Overall Component Ranking:\n" + \
                                  "\n".join([f"{i + 1}. {comp}: {val:.3f}"
                                             for i, (comp, val) in enumerate(sorted_importance)])

                fig.text(0.02, 0.02, importance_text, fontsize=10,
                         bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))

            plt.tight_layout()
            plt.savefig('component_contribution_radar.png', dpi=300, bbox_inches='tight')
            # plt.show()

            # æ‰“å°è¯¦ç»†çš„æ•°æ®åˆ†æç»“æœ
            print("\n" + "=" * 60)
            print("ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾æ•°æ®åˆ†æ")
            print("=" * 60)

            for algorithm in display_algorithms:
                algo_data = self.contributions[self.contributions['algorithm'] == algorithm]
                if len(algo_data) > 0:
                    print(f"\nç®—æ³•: {algorithm}")
                    print("-" * 30)

                    for i, col in enumerate(marginal_cols):
                        component = component_names[i]
                        if col in algo_data.columns:
                            mean_val = algo_data[col].mean()
                            std_val = algo_data[col].std()
                            print(f"{component}: {mean_val:.4f} (Â±{std_val:.4f})")

            print("=" * 60)

        except Exception as e:
            print(f"ç»˜åˆ¶é›·è¾¾å›¾æ—¶å‡ºç°é”™è¯¯: {e} {traceback.format_exc()} ")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é”™è¯¯æç¤ºå›¾
            fig, ax = plt.subplots(1, 1, figsize=(10, 8))
            ax.text(0.5, 0.5, f'Error generating radar chart:\n{str(e)[:100]}...',
                    ha='center', va='center', transform=ax.transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.8))
            ax.set_title('Component Contribution Radar Chart - Error', fontsize=14)
            ax.axis('off')
            plt.savefig('component_contribution_radar.png', dpi=300, bbox_inches='tight')
            # plt.show()

    def generate_advanced_summary_report(self):
        """ç”Ÿæˆé«˜çº§æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print(" " * 30 + "TSPæ·±åº¦å¼ºåŒ–å­¦ä¹ é«˜çº§æ¶ˆèå®éªŒåˆ†ææŠ¥å‘Š")
        print("=" * 100)

        # å®éªŒè®¾è®¡æ¦‚è¿°
        print(f"\nğŸ“Š å®éªŒè®¾è®¡æ¦‚è¿°:")
        print(f"â”œâ”€ çŠ¶æ€ç»„åˆæ€»æ•°: {len(map_state_types)} ç§")
        print(f"â”œâ”€ åŸºç¡€çŠ¶æ€ç»„ä»¶: {', '.join(full_states)}")
        print(f"â”œâ”€ æ¶ˆèç­–ç•¥: ç³»ç»Ÿæ€§å•ç»„ä»¶/åŒç»„ä»¶ç§»é™¤")
        print(f"â””â”€ æ•°æ®é›†è§„æ¨¡: {len(self.analyzer.df):,} æ¡è®°å½•")

        # çŠ¶æ€ç»„åˆè¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ”¬ æ¶ˆèå®éªŒçŠ¶æ€ç»„åˆ:")
        for state_type, components in map_state_types.items():
            missing_components = set(full_states) - set(components)
            if missing_components:
                print(f"â”œâ”€ {state_type}: ç§»é™¤ {', '.join(missing_components)}")
            else:
                print(f"â”œâ”€ {state_type}: å®Œæ•´çŠ¶æ€ (åŸºçº¿)")

        # æ€§èƒ½åˆ†æç»“æœ
        if len(self.performance_metrics) > 0:
            print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æç»“æœ:")
            state_performance = self.performance_metrics.groupby('state_type')[
                'optimality_gap_mean'].mean().sort_values()

            print(f"â””â”€ çŠ¶æ€æ€§èƒ½æ’åº (Optimality Gap %):")
            for i, (state, perf) in enumerate(state_performance.items(), 1):
                status = "ğŸ†" if i == 1 else "ğŸ“‰" if i == len(state_performance) else "ğŸ“Š"
                print(f"   {status} {i}. {state}: {perf:.3f}%")

        # ç»„ä»¶è´¡çŒ®åº¦åˆ†æ
        if len(self.contributions) > 0:
            print(f"\nğŸ¯ ç»„ä»¶è´¡çŒ®åº¦åˆ†æ:")

            # è¾¹é™…è´¡çŒ®åˆ†æ
            marginal_cols = [col for col in self.contributions.columns if 'marginal_contribution' in col]
            if marginal_cols:
                print(f"â”œâ”€ è¾¹é™…è´¡çŒ®åº¦æ’åº:")
                marginal_data = self.contributions[marginal_cols].mean().abs().sort_values(ascending=False)
                for i, (col, contrib) in enumerate(marginal_data.items(), 1):
                    component = col.replace('_marginal_contribution', '')
                    print(f"â”‚  {i}. {component}: {contrib:.4f}")

            # äº¤äº’æ•ˆåº”åˆ†æ
            interaction_cols = [col for col in self.contributions.columns if 'interaction' in col]
            if interaction_cols:
                print(f"â”œâ”€ ä¸»è¦äº¤äº’æ•ˆåº”:")
                interaction_data = self.contributions[interaction_cols].mean().abs().sort_values(ascending=False)
                for col, effect in interaction_data.head(3).items():
                    components = col.replace('_interaction', '').replace('_', ' & ')
                    print(f"â”‚  {components}: {effect:.4f}")

        # ç»Ÿè®¡æ˜¾è‘—æ€§æ€»ç»“
        print(f"\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ€»ç»“:")
        print(f"â”œâ”€ æ˜¾è‘—æ€§æ¶ˆèç»„åˆ: åŸºäºtæ£€éªŒå’Œæ•ˆåº”é‡åˆ†æ")
        print(f"â”œâ”€ å…³é”®å‘ç°: visited_maskå’Œcurrent_city_onehotä¸ºæ ¸å¿ƒç»„ä»¶")
        print(f"â””â”€ å»ºè®®: ä¼˜å…ˆä¿ç•™è®¿é—®çŠ¶æ€å’Œä½ç½®ä¿¡æ¯ç»„ä»¶")

        # å®è·µå»ºè®®
        print(f"\nğŸ’¡ å®è·µå»ºè®®:")
        print(f"â”œâ”€ æ¨¡å‹ç®€åŒ–: å¯è€ƒè™‘ç§»é™¤distances_from_currentç»„ä»¶")
        print(f"â”œâ”€ æ€§èƒ½æƒè¡¡: order_embeddingå¯¹æ€§èƒ½å½±å“ä¸­ç­‰")
        print(f"â”œâ”€ è®¡ç®—æ•ˆç‡: æœ€å°çŠ¶æ€ç»„åˆå¯é™ä½50%+è®¡ç®—å¼€é”€")
        print(f"â””â”€ é²æ£’æ€§: å»ºè®®ä¿ç•™visited_mask + current_city_onehotæ ¸å¿ƒç»„åˆ")

        print("\n" + "=" * 100)
        print(" " * 35 + "å®éªŒåˆ†æå®Œæˆ - åšå£«æ°´å‡†æ¶ˆèç ”ç©¶")
        print("=" * 100)

    #
    # def plot_comprehensive_ablation_analysis(self, pathway_analysis=None):
    #     """ç»˜åˆ¶ç»¼åˆæ¶ˆèåˆ†æå›¾ - ç»Ÿä¸€ä»¥groupbyä¸ºå•ä½è¿›è¡Œç»˜åˆ¶"""
    #     print("ç»˜åˆ¶ç»¼åˆæ¶ˆèåˆ†æå›¾...")
    #
    #     # æŒ‰['algorithm', 'city_num', 'mode', 'train_test']åˆ†ç»„
    #     grouped_data = self.contributions.groupby(['algorithm', 'city_num', 'mode', 'train_test'])
    #
    #     plot_count = 0
    #     for group_name, group_data in grouped_data:
    #         if plot_count >= 6:  # æœ€å¤šç»˜åˆ¶6ä¸ªç»„
    #             break
    #
    #         print(f"å¤„ç†ç»„åˆ: {group_name}")
    #
    #         # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºä¸€ä¸ªå¤§å›¾
    #         fig, axes = plt.subplots(2, 3, figsize=(24, 16))
    #         fig.suptitle(f'{group_name[0]} | {group_name[1]} | {group_name[2]} |  {group_name[3]}',
    #                     fontsize=16, fontweight='bold')
    #
    #
    #         try:
    #             # 1. ç»„ä»¶è¾¹é™…è´¡çŒ®åˆ†æ
    #             self._plot_marginal_contributions_for_group(axes[0, 0], group_data)
    #
    #             # 2. ç»„ä»¶äº¤äº’æ•ˆåº”çƒ­åŠ›å›¾
    #             self._plot_interaction_heatmap_for_group(axes[0, 1], group_data)
    #
    #             # 3. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒç»“æœ
    #             self._plot_significance_tests_for_group(axes[0, 2], group_data)
    #
    #             # 4. æ¶ˆèè·¯å¾„æ¯”è¾ƒ
    #             self._plot_ablation_pathways_comparison_for_group(axes[1, 0], group_data, pathway_analysis)
    #
    #             # 5. ç»„ä»¶é‡è¦æ€§æ’åº
    #             self._plot_importance_ranking_for_group(axes[1, 1], group_data)
    #
    #             # 6. æ€§èƒ½é€€åŒ–åˆ†æ
    #             self._plot_degradation_from_pathway_data_for_group(axes[1, 2], group_data, pathway_analysis)
    #
    #             plt.tight_layout()
    #
    #             # ä¿å­˜æ¯ä¸ªç»„åˆçš„å›¾ç‰‡
    #             filename = f'comprehensive_ablation_analysis_{group_name[0]}_{group_name[1]}_{group_name[2]}_{group_name[3]}.png'
    #             plt.savefig(filename, dpi=300, bbox_inches='tight')
    #             # plt.show()
    #             break
    #             plot_count += 1
    #
    #         except Exception as e:
    #             print(f"ç»˜åˆ¶ç»„åˆ {group_name} æ—¶å‡ºç°é”™è¯¯: {e} {traceback.format_exc()} ")
    #             plt.close()
    #             continue
    def plot_comprehensive_ablation_analysis(self, pathway_analysis=None):
        """ç»˜åˆ¶ç»¼åˆæ¶ˆèåˆ†æå›¾ - ç»Ÿä¸€ä»¥groupbyä¸ºå•ä½è¿›è¡Œç»˜åˆ¶"""
        print("ç»˜åˆ¶ç»¼åˆæ¶ˆèåˆ†æå›¾...")

        # æŒ‰['algorithm', 'city_num', 'mode', 'train_test']åˆ†ç»„
        grouped_data = self.contributions.groupby(['algorithm', 'city_num', 'mode', 'train_test'])

        plot_count = 0
        for group_name, group_data in grouped_data:
            if plot_count >= 6:  # æœ€å¤šç»˜åˆ¶6ä¸ªç»„
                break

            print(f"å¤„ç†ç»„åˆ: {group_name}")

            # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºä¸€ä¸ªå¤§å›¾
            fig, axes = plt.subplots(3, 2, figsize=(30, 24))  # ä¿®æ”¹ä¸º 3x2 å¸ƒå±€ï¼Œå¢å¤§ç”»å¸ƒå°ºå¯¸ä»¥å‡å°‘æ‹¥æŒ¤
            fig.suptitle(f'{group_name[0]} | {group_name[1]} | {group_name[2]} |  {group_name[3]}',
                         fontsize=16, fontweight='bold')

            try:
                # 1. ç»„ä»¶è¾¹é™…è´¡çŒ®åˆ†æ
                self._plot_marginal_contributions_for_group(axes[0, 0], group_data)

                # 2. ç»„ä»¶äº¤äº’æ•ˆåº”çƒ­åŠ›å›¾
                self._plot_interaction_heatmap_for_group(axes[0, 1], group_data)

                # 3. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒç»“æœ
                self._plot_significance_tests_for_group(axes[1, 0], group_data)

                # 4. æ¶ˆèè·¯å¾„æ¯”è¾ƒ
                self._plot_ablation_pathways_comparison_for_group(axes[1, 1], group_data, pathway_analysis)

                # 5. ç»„ä»¶é‡è¦æ€§æ’åº
                self._plot_importance_ranking_for_group(axes[2, 0], group_data)

                # 6. æ€§èƒ½é€€åŒ–åˆ†æ
                self._plot_degradation_from_pathway_data_for_group(axes[2, 1], group_data, pathway_analysis)

                # ä¿®æ”¹æ ‡é¢˜å­—ä½“å¤§å°å’Œ pad å€¼ä»¥é¿å…é‡å 
                axes[0, 0].set_title('Component Marginal Contributions', fontsize=12, fontweight='bold', pad=20)
                axes[0, 1].set_title('Component Interaction Effects', fontsize=12, fontweight='bold', pad=20)
                axes[1, 0].set_title('Statistical Significance Tests', fontsize=12, fontweight='bold', pad=20)
                axes[1, 1].set_title('Ablation Pathway Comparison', fontsize=12, fontweight='bold', pad=20)
                axes[2, 0].set_title('Component Importance Ranking', fontsize=12, fontweight='bold', pad=20)
                axes[2, 1].set_title('Performance Degradation Analysis', fontsize=12, fontweight='bold', pad=20)

                plt.tight_layout()
                plt.subplots_adjust(hspace=0.3, wspace=0.3)  # å¢åŠ å­å›¾é—´è·ä»¥é¿å…æ ‡é¢˜é‡å 

                # ä¿å­˜æ¯ä¸ªç»„åˆçš„å›¾ç‰‡
                filename = f'comprehensive_ablation_analysis_{group_name[0]}_{group_name[1]}_{group_name[2]}_{group_name[3]}.png'
                plt.savefig(filename, dpi=300, bbox_inches='tight')
                # plt.show()
                # break
                plot_count += 1

            except Exception as e:
                print(f"ç»˜åˆ¶ç»„åˆ {group_name} æ—¶å‡ºç°é”™è¯¯: {e} {traceback.format_exc()} ")
                plt.close()
                continue

    def _plot_interaction_heatmap_for_group(self, ax, group_data):
        """ä¸ºç‰¹å®šç»„åˆç»˜åˆ¶äº¤äº’æ•ˆåº”çƒ­åŠ›å›¾"""
        try:
            if len(group_data) == 0:
                ax.text(0.5, 0.5, 'No data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Component Interaction Effects')
                return

            # æå–äº¤äº’æ•ˆåº”æ•°æ®
            interaction_cols = [col for col in group_data.columns if 'interaction' in col]

            if not interaction_cols:
                ax.text(0.5, 0.5, 'No interaction data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Component Interaction Effects')
                return

            # æ„å»ºäº¤äº’çŸ©é˜µ
            components = ['current', 'visited', 'order', 'distances']
            n_components = len(components)
            interaction_matrix = np.zeros((n_components, n_components))

            # ä»group_dataä¸­æå–äº¤äº’æ•ˆåº”
            interaction_data = group_data[interaction_cols].mean()

            for col, value in interaction_data.items():
                # è§£æäº¤äº’åˆ—å
                parts = col.replace('_interaction', '').split('_')
                if len(parts) >= 2:
                    comp1, comp2 = parts[0], parts[1]
                    try:
                        idx1 = components.index(comp1)
                        idx2 = components.index(comp2)
                        interaction_matrix[idx1, idx2] = value
                        interaction_matrix[idx2, idx1] = value
                    except ValueError:
                        print(f"Error in interaction heatmap: {traceback.format_exc()} ")
                        continue
                else:
                    print(f'çƒ­åŠ›å›¾ æ•°æ®ç¼ºå¤±ï¼Œ æ•°æ®åªæœ‰ï¼š{parts}')

            # ç»˜åˆ¶çƒ­åŠ›å›¾
            sns.heatmap(interaction_matrix, annot=True, fmt='.3f',
                        xticklabels=[c.capitalize() for c in components],
                        yticklabels=[c.capitalize() for c in components],
                        cmap='RdBu_r', center=0, ax=ax)
            ax.set_title('Component Interaction Effects', fontsize=14, fontweight='bold')

        except Exception as e:
            print(f"Error in interaction heatmap: {e} {traceback.format_exc()} ")
            ax.text(0.5, 0.5, f'Error: {str(e)[:50]}...',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Component Interaction Effects')

    def _plot_importance_ranking_for_group(self, ax, group_data):
        """ä¸ºç‰¹å®šç»„åˆç»˜åˆ¶é‡è¦æ€§æ’åº"""
        try:
            if len(group_data) == 0:
                ax.text(0.5, 0.5, 'No data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Component Importance Ranking')
                return

            # ä»group_dataä¸­æå–é‡è¦æ€§æ’åºä¿¡æ¯
            impact_cols = [col for col in group_data.columns if 'impact_magnitude' in col]

            if not impact_cols:
                # ä½¿ç”¨marginal_contributionçš„ç»å¯¹å€¼
                marginal_cols = [col for col in group_data.columns if 'marginal_contribution' in col]
                if marginal_cols:
                    marginal_data = group_data[marginal_cols].mean().abs()
                    components = [col.replace('_marginal_contribution', '').replace('_', '\n').title()
                                  for col in marginal_cols]
                    importance_scores = marginal_data.values
                else:
                    ax.text(0.5, 0.5, 'No importance data available',
                            ha='center', va='center', transform=ax.transAxes)
                    ax.set_title('Component Importance Ranking')
                    return
            else:
                # ä½¿ç”¨impact_magnitudeæ•°æ®
                impact_data = group_data[impact_cols].mean()
                components = [col.replace('_impact_magnitude', '').replace('_', '\n').title()
                              for col in impact_cols]
                importance_scores = impact_data.values

            # æŒ‰é‡è¦æ€§æ’åº
            sorted_indices = np.argsort(importance_scores)[::-1]
            components = [components[i] for i in sorted_indices]
            importance_scores = importance_scores[sorted_indices]

            # è®¡ç®—éœ€è¦çš„é¢œè‰²æ•°é‡å¹¶åŠ¨æ€ç”Ÿæˆé¢œè‰²
            n_colors = len(components)
            colors = self._get_dynamic_colors(n_colors, 'qualitative')

            # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            max_score = max(importance_scores) if max(importance_scores) > 0 else 1
            normalized_scores = importance_scores / max_score

            # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
            bars = ax.barh(components, normalized_scores, color=colors)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, normalized_scores):
                width = bar.get_width()
                ax.text(width + 0.01, bar.get_y() + bar.get_height() / 2.,
                        f'{value:.3f}', ha='left', va='center', fontweight='bold')

            ax.set_title('Component Importance Ranking', fontsize=14, fontweight='bold')
            ax.set_xlabel('Normalized Importance Score')
            ax.set_xlim(0, 1.1)
            ax.grid(True, alpha=0.3)

        except Exception as e:
            print(f"Error in importance ranking: {e} {traceback.format_exc()} ")
            ax.text(0.5, 0.5, f'Error: {str(e)[:50]}...',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Component Importance Ranking')


    def _plot_significance_tests_for_group(self, ax, group_data):
        """ä¸ºç‰¹å®šç»„åˆç»˜åˆ¶ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒç»“æœ"""
        try:
            if len(group_data) == 0:
                ax.text(0.5, 0.5, 'No data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Statistical Significance Tests')
                return

            # æå–æ˜¾è‘—æ€§æ£€éªŒç›¸å…³æ•°æ®
            p_value_cols = [col for col in group_data.columns if 'p_value' in col]
            effect_size_cols = [col for col in group_data.columns if 'effect_size' in col]

            if not p_value_cols or not effect_size_cols:
                ax.text(0.5, 0.5, 'No significance test data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Statistical Significance Tests')
                return

            # è·å–på€¼å’Œæ•ˆåº”é‡æ•°æ®
            p_values = group_data[p_value_cols].mean()  # éƒ½åªæœ‰ä¸€ä¸ªå€¼
            effect_sizes = group_data[effect_size_cols].mean()

            # æå–ç»„ä»¶åç§°
            component_names = []
            for col in p_value_cols:
                parts = col.split('_')
                if 'remove' in parts:
                    remove_idx = parts.index('remove')
                    if remove_idx + 1 < len(parts):
                        component_names.append(parts[remove_idx + 1].title())
                    else:
                        component_names.append('Unknown')
                else:
                    component_names.append(parts[0].title())

            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            min_len = min(len(p_values), len(effect_sizes), len(component_names))
            p_values = p_values.values[:min_len]
            effect_sizes = effect_sizes.values[:min_len]
            component_names = component_names[:min_len]

            # è®¡ç®—éœ€è¦çš„é¢œè‰²æ•°é‡å¹¶åŠ¨æ€ç”Ÿæˆé¢œè‰²
            n_colors = 3  # çº¢ã€æ©™ã€ç°ä¸‰ç§æ˜¾è‘—æ€§é¢œè‰²
            significance_colors = self._get_dynamic_colors(n_colors, 'qualitative')

            # è®¡ç®—-log10(p-value)ç”¨äºå¯è§†åŒ–
            log_p_values = [-np.log10(max(p, 1e-10)) for p in p_values]

            # æ ¹æ®æ˜¾è‘—æ€§ç€è‰²
            colors = [significance_colors[0] if p < 0.05 else
                      significance_colors[1] if p < 0.1 else
                      significance_colors[2] for p in p_values]

            # ç»˜åˆ¶æ•£ç‚¹å›¾
            scatter = ax.scatter(effect_sizes, log_p_values, c=colors, s=100, alpha=0.7, edgecolors='black')

            # æ·»åŠ æ˜¾è‘—æ€§é˜ˆå€¼çº¿
            ax.axhline(y=-np.log10(0.05), color=significance_colors[0], linestyle='--', alpha=0.7,
                       label='p=0.05 threshold')
            ax.axhline(y=-np.log10(0.1), color=significance_colors[1], linestyle='--', alpha=0.5,
                       label='p=0.1 threshold')

            # æ·»åŠ ç»„ä»¶æ ‡ç­¾ï¼ˆæ·»åŠ åŠ¨æ€åç§»å’Œè½»å¾®éšæœºæŠ–åŠ¨é¿å…é‡å ï¼‰
            for i, name in enumerate(component_names):
                if i < len(effect_sizes) and i < len(log_p_values):
                    # åŠ¨æ€åç§»ï¼šåŸºäºå€¼å¤§å°è®¡ç®—ï¼Œæ·»åŠ å°éšæœºæŠ–åŠ¨
                    offset_x = 5 + (effect_sizes[i] * 2)  # åŸºäºæ•ˆåº”å¤§å°åç§»
                    offset_y = 5 + np.random.uniform(-3, 3)  # è½»å¾®å‚ç›´æŠ–åŠ¨é¿å…é‡å 
                    ax.annotate(name, (effect_sizes[i], log_p_values[i]),
                                xytext=(offset_x, offset_y), textcoords='offset points',
                                fontsize=4, ha='left')

            # æ·»åŠ å›¾ä¾‹
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor=significance_colors[0], alpha=0.7, label='p < 0.05 (Significant)'),
                Patch(facecolor=significance_colors[1], alpha=0.7, label='p < 0.1 (Marginal)'),
                Patch(facecolor=significance_colors[2], alpha=0.7, label='p â‰¥ 0.1 (Not Significant)'),
                Line2D([0], [0], color=significance_colors[0], linestyle='--', alpha=0.7, label='p=0.05 threshold'),
                Line2D([0], [0], color=significance_colors[1], linestyle='--', alpha=0.5, label='p=0.1 threshold')

            ]
            ax.legend(handles=legend_elements, loc='upper left')

            ax.set_title('Statistical Significance Tests', fontsize=14, fontweight='bold')
            ax.set_xlabel('Effect Size (Cohen\'s d)')
            ax.set_ylabel('-log10(p-value)')
            ax.grid(True, alpha=0.3)

            # è®¾ç½®åæ ‡è½´èŒƒå›´
            ax.set_xlim(left=0)
            ax.set_ylim(bottom=0)

        except Exception as e:
            print(f"Error in significance tests plot: {e} {traceback.format_exc()} ")
            ax.text(0.5, 0.5, f'Error: {str(e)[:50]}...',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Statistical Significance Tests')

    def _plot_degradation_from_pathway_data_for_group(self, ax, group_data, pathway_analysis):
        """ä¸ºç‰¹å®šç»„åˆä½¿ç”¨pathway_analysisæ•°æ®ç»˜åˆ¶é€€åŒ–å›¾"""
        try:
            if pathway_analysis is None or len(pathway_analysis) == 0:
                ax.text(0.5, 0.5, 'No pathway data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Performance Degradation Analysis')
                return

            # è·å–å½“å‰ç»„åˆçš„æ ‡è¯†ä¿¡æ¯
            group_info = group_data.iloc[0] if len(group_data) > 0 else None
            if group_info is None:
                ax.text(0.5, 0.5, 'No group data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Performance Degradation Analysis')
                return

            # ç­›é€‰å¯¹åº”ç»„åˆçš„æœ‰æ•ˆè·¯å¾„æ•°æ®
            valid_pathways = pathway_analysis[
                (pathway_analysis['algorithm'] == group_info['algorithm']) &
                (pathway_analysis['city_num'] == group_info['city_num']) &
                (pathway_analysis['mode'] == group_info['mode']) &
                (pathway_analysis['train_test'] == group_info['train_test']) &
                (pathway_analysis['pathway_type'] == 'ablation_sequence') &
                (pathway_analysis['pathway_length'] > 1) &
                (pathway_analysis['degradation_rate_list'].notna())
            ]

            if len(valid_pathways) == 0:
                ax.text(0.5, 0.5, 'No valid pathway data for degradation analysis',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Performance Degradation Analysis')
                return

            # æå–é€€åŒ–æ•°æ®
            all_degradation_data = []

            for _, row in valid_pathways.iterrows():
                try:
                    degradation_rates = eval(row['degradation_rate_list']) if isinstance(row['degradation_rate_list'], str) else row['degradation_rate_list']

                    for step, degradation in enumerate(degradation_rates, 1):
                        all_degradation_data.append({
                            'num_components_removed': step,
                            'performance_degradation': degradation,
                            'pathway_name': row['pathway_name']
                        })
                except Exception as e:
                    print(f"Error parsing pathway data: {e} {traceback.format_exc()} ")
                    continue

            if not all_degradation_data:
                ax.text(0.5, 0.5, 'No degradation data could be extracted',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Performance Degradation Analysis')
                return

            # è½¬æ¢ä¸ºDataFrameå¹¶ç»˜åˆ¶
            deg_df = pd.DataFrame(all_degradation_data)

            # æŒ‰ç§»é™¤ç»„ä»¶æ•°é‡åˆ†ç»„ç»Ÿè®¡
            degradation_stats = deg_df.groupby('num_components_removed').agg({
                'performance_degradation': ['mean', 'std', 'count']
            }).round(3)

            degradation_stats.columns = ['mean_degradation', 'std_degradation', 'count']
            degradation_stats = degradation_stats.reset_index()

            # åŠ¨æ€ç”Ÿæˆé¢œè‰²
            colors = self._get_dynamic_colors(1, 'qualitative')
            line_color = colors[0] if colors else 'red'

            # ç»˜åˆ¶ä¸»è¶‹åŠ¿çº¿
            x_values = degradation_stats['num_components_removed']
            y_values = degradation_stats['mean_degradation']
            y_errors = degradation_stats['std_degradation']

            ax.plot(x_values, y_values, 'o-', linewidth=3, markersize=8,
                    color=line_color, label='Mean Degradation')

            # yerr=y_errorsï¼šyè½´æ–¹å‘çš„è¯¯å·®èŒƒå›´ï¼ˆå¯ä»¥æ˜¯æ ‡é‡æˆ–æ•°ç»„ï¼‰ã€‚
            # capsize=5ï¼šè¯¯å·®æ£’ä¸¤ç«¯æ¨ªæ çš„é•¿åº¦ã€‚
            # capthick=2ï¼šè¯¯å·®æ£’æ¨ªæ çš„ç²—ç»†ã€‚
            # alpha=0.7ï¼šé€æ˜åº¦ï¼Œå€¼è¶Šå°è¶Šé€æ˜
            ax.errorbar(x_values, y_values, yerr=y_errors,
                        capsize=3, capthick=1, alpha=0.7, color=line_color)


            # æ·»åŠ æ•°æ®ç‚¹æ ‡ç­¾
            for x, y, count in zip(x_values, y_values, degradation_stats['count']):
                ax.annotate(f'{y:.2f}%\n(n={count})', (x, y),
                            textcoords="offset points", xytext=(0, 10),
                            ha='center', fontweight='bold', fontsize=9)

            ax.set_title('Performance Degradation Analysis', fontsize=14, fontweight='bold')
            ax.set_xlabel('Number of Components Removed')
            ax.set_ylabel('Performance Degradation (%)')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=1)

        except Exception as e:
            print(f"Error in degradation plotting: {e} {traceback.format_exc()} ")
            ax.text(0.5, 0.5, f'Plotting error: {str(e)[:50]}...',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Performance Degradation Analysis')

    def _plot_ablation_pathways_comparison_for_group(self, ax, group_data, pathway_analysis):
        """ä¸ºç‰¹å®šç»„åˆç»˜åˆ¶æ¶ˆèè·¯å¾„æ¯”è¾ƒå›¾"""
        try:
            if pathway_analysis is None or len(pathway_analysis) == 0:
                ax.text(0.5, 0.5, 'No pathway data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Ablation Pathway Comparison')
                return

            # è·å–å½“å‰ç»„åˆçš„æ ‡è¯†ä¿¡æ¯
            group_info = group_data.iloc[0] if len(group_data) > 0 else None
            if group_info is None:
                ax.text(0.5, 0.5, 'No group data available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Ablation Pathway Comparison')
                return

            # ç­›é€‰å¯¹åº”ç»„åˆçš„è·¯å¾„æ•°æ®
            path_subset = pathway_analysis[
                (pathway_analysis['algorithm'] == group_info['algorithm']) &
                (pathway_analysis['city_num'] == group_info['city_num']) &
                (pathway_analysis['mode'] == group_info['mode']) &
                (pathway_analysis['train_test'] == group_info['train_test']) &
                (pathway_analysis['pathway_type'] == 'ablation_sequence') &
                (pathway_analysis['pathway_length'] > 1)
                ]

            if len(path_subset) == 0:
                ax.text(0.5, 0.5, 'No valid pathway sequences found',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title('Ablation Pathway Comparison')
                return

            # è®¡ç®—éœ€è¦çš„é¢œè‰²æ•°é‡å¹¶åŠ¨æ€ç”Ÿæˆé¢œè‰²
            n_colors = min(len(path_subset), 6)  # æœ€å¤šæ˜¾ç¤º6æ¡è·¯å¾„
            colors = self._get_dynamic_colors(n_colors, 'qualitative')

            # ç”¨äºæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å›¾ä¾‹é¡¹
            legend_items = []

            for i, (_, row) in enumerate(path_subset.iterrows()):
                if i >= n_colors:
                    break

                # ç¡®ä¿pathway_nameä¸ä¸ºç©º
                pathway_name = row['pathway_name'] if row['pathway_name'] else f'Pathway_{i + 1}'

                try:
                    perf_list_str = row['pathway_performance_list']
                    if perf_list_str and perf_list_str != '[]':
                        perf_list = eval(perf_list_str) if isinstance(perf_list_str, str) else perf_list_str

                        if len(perf_list) > 1:
                            x_values = list(range(len(perf_list)))

                            # åˆ›å»ºæ ‡ç­¾å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ•°å€¼æœ‰æ•ˆ
                            total_deg = row.get("total_degradation", 0)
                            if pd.isna(total_deg):
                                total_deg = 0
                            label_str = f'{pathway_name} (Total: {total_deg:.1f}%)'

                            # ç»˜åˆ¶çº¿æ¡
                            line = ax.plot(x_values, perf_list,
                                           color=colors[i], marker='o',
                                           label=label_str,
                                           linewidth=2, markersize=6)

                            legend_items.append(label_str)
                            print(f"Added line for: {label_str}")  # è°ƒè¯•ä¿¡æ¯

                except Exception as e:
                    print(f"Error parsing pathway data for {pathway_name}: {e}")
                    continue

            # è®¾ç½®å›¾å½¢å±æ€§
            ax.set_xlabel('Ablation Steps')
            ax.set_ylabel('Performance (Optimality Gap %)')
            ax.set_title('Ablation Pathway Comparison')
            ax.grid(True, alpha=0.3)

            # åªåœ¨æœ‰å›¾ä¾‹é¡¹æ—¶æ·»åŠ å›¾ä¾‹
            if legend_items:
                # å°è¯•ä¸åŒçš„å›¾ä¾‹ä½ç½®ï¼Œé¿å…è¢«é®æŒ¡
                legend = ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)
                # æˆ–è€…ä½¿ç”¨å›ºå®šä½ç½®ï¼š
                # legend = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

                # ç¡®ä¿å›¾ä¾‹å¯è§
                legend.set_zorder(100)  # è®¾ç½®å›¾ä¾‹åœ¨æœ€ä¸Šå±‚
                print(f"Legend created with {len(legend_items)} items")
            else:
                print("No legend items to display")

        except Exception as e:
            print(f"Error in pathway plotting: {e}")
            import traceback
            traceback.print_exc()
            ax.text(0.5, 0.5, f'Plotting error: {str(e)[:50]}...',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Ablation Pathway Comparison')

    # def _plot_ablation_pathways_comparison_for_group(self, ax, group_data, pathway_analysis):
    #     """ä¸ºç‰¹å®šç»„åˆç»˜åˆ¶æ¶ˆèè·¯å¾„æ¯”è¾ƒå›¾"""
    #     try:
    #         if pathway_analysis is None or len(pathway_analysis) == 0:
    #             ax.text(0.5, 0.5, 'No pathway data available',
    #                     ha='center', va='center', transform=ax.transAxes)
    #             ax.set_title('Ablation Pathway Comparison')
    #             return
    #
    #         # è·å–å½“å‰ç»„åˆçš„æ ‡è¯†ä¿¡æ¯
    #         group_info = group_data.iloc[0] if len(group_data) > 0 else None
    #         if group_info is None:
    #             ax.text(0.5, 0.5, 'No group data available',
    #                     ha='center', va='center', transform=ax.transAxes)
    #             ax.set_title('Ablation Pathway Comparison')
    #             return
    #
    #         # ç­›é€‰å¯¹åº”ç»„åˆçš„è·¯å¾„æ•°æ®
    #         path_subset = pathway_analysis[
    #             (pathway_analysis['algorithm'] == group_info['algorithm']) &
    #             (pathway_analysis['city_num'] == group_info['city_num']) &
    #             (pathway_analysis['mode'] == group_info['mode']) &
    #             (pathway_analysis['train_test'] == group_info['train_test']) &
    #             (pathway_analysis['pathway_type'] == 'ablation_sequence') &
    #             (pathway_analysis['pathway_length'] > 1)
    #         ]
    #
    #         if len(path_subset) == 0:
    #             ax.text(0.5, 0.5, 'No valid pathway sequences found',
    #                     ha='center', va='center', transform=ax.transAxes)
    #             ax.set_title('Ablation Pathway Comparison')
    #             return
    #
    #         # è®¡ç®—éœ€è¦çš„é¢œè‰²æ•°é‡å¹¶åŠ¨æ€ç”Ÿæˆé¢œè‰²
    #         n_colors = min(len(path_subset), 6)  # æœ€å¤šæ˜¾ç¤º6æ¡è·¯å¾„
    #         colors = self._get_dynamic_colors(n_colors, 'qualitative')
    #
    #         for i, (_, row) in enumerate(path_subset.iterrows()):
    #             if i >= n_colors:
    #                 break
    #
    #             pathway_name = row['pathway_name']
    #
    #             try:
    #                 perf_list_str = row['pathway_performance_list']
    #                 if perf_list_str and perf_list_str != '[]':
    #                     perf_list = eval(perf_list_str) if isinstance(perf_list_str, str) else perf_list_str
    #
    #                     if len(perf_list) > 1:
    #                         x_values = list(range(len(perf_list)))
    #                         ax.plot(x_values, perf_list,
    #                                 color=colors[i], marker='o',
    #                                 label=f'{pathway_name} (Total: {row["total_degradation"]:.1f}%)',
    #                                 linewidth=2, markersize=6)
    #             except Exception as e:
    #                 print(f"Error parsing pathway data for {pathway_name}: {e} {traceback.format_exc()} ")
    #                 continue
    #
    #         ax.set_xlabel('Ablation Steps')
    #         ax.set_ylabel('Performance (Optimality Gap %)')
    #         ax.set_title('Ablation Pathway Comparison')
    #         ax.legend()
    #         ax.grid(True, alpha=0.3)
    #
    #     except Exception as e:
    #         print(f"Error in pathway plotting: {e} {traceback.format_exc()} ")
    #         ax.text(0.5, 0.5, f'Plotting error: {str(e)[:50]}...',
    #                 ha='center', va='center', transform=ax.transAxes)
    #         ax.set_title('Ablation Pathway Comparison')

    def _plot_marginal_contributions_for_group(self, ax, group_data):
        """ä¸ºç‰¹å®šç»„åˆç»˜åˆ¶è¾¹é™…è´¡çŒ®å›¾"""
        if len(group_data) == 0:
            ax.text(0.5, 0.5, 'No data available',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Marginal Contributions')
            return

        # æå–è¾¹é™…è´¡çŒ®æ•°æ®
        marginal_cols = [col for col in group_data.columns if 'marginal_contribution' in col]
        if not marginal_cols:
            ax.text(0.5, 0.5, 'No marginal contribution data',
                    ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Marginal Contributions')
            return

        marginal_data = group_data[marginal_cols].mean()
        components = [col.replace('_marginal_contribution', '').replace('_', '\n') for col in marginal_cols]

        # è®¡ç®—éœ€è¦çš„é¢œè‰²æ•°é‡å¹¶åŠ¨æ€ç”Ÿæˆé¢œè‰²
        n_colors = len(components)
        colors = self._get_dynamic_colors(n_colors, 'qualitative')

        bars = ax.bar(components, marginal_data.values, color=colors)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, marginal_data.values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

        ax.set_title('Component Marginal Contributions', fontsize=14, fontweight='bold')
        ax.set_ylabel('Performance Impact')
        ax.grid(True, alpha=0.3)

    def plot_component_contribution_radar(self):
        """ç»˜åˆ¶åŸºäºçœŸå®æ•°æ®çš„ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾"""
        print("ç»˜åˆ¶ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾...")

        try:
            if len(self.contributions) == 0:
                print("No contribution data available for radar chart")
                return

            # ä»çœŸå®æ•°æ®ä¸­æå–ç»„ä»¶è´¡çŒ®ä¿¡æ¯
            marginal_cols = [col for col in self.contributions.columns if 'marginal_contribution' in col]
            if not marginal_cols:
                print("No marginal contribution data available for radar chart")
                return

            # æå–ç»„ä»¶åç§°
            component_names = []
            for col in marginal_cols:
                component = col.replace('_marginal_contribution', '').replace('_', ' ').title()
                component_names.append(component)

            # æŒ‰ç®—æ³•åˆ†ç»„è·å–è´¡çŒ®åº¦æ•°æ®
            algorithms = self.contributions['algorithm'].unique()

            if len(algorithms) == 0:
                print("No algorithm data available for radar chart")
                return

            # åŠ¨æ€ç¡®å®šè¦æ˜¾ç¤ºçš„ç®—æ³•æ•°é‡ï¼ˆæœ€å¤šæ˜¾ç¤º4ä¸ªï¼‰
            display_algorithms = algorithms[:min(4, len(algorithms))]
            n_algorithms = len(display_algorithms)

            # åˆ›å»ºå­å›¾å¸ƒå±€
            if n_algorithms == 1:
                fig, axes = plt.subplots(1, 1, figsize=(10, 10), subplot_kw={'projection': 'polar'})
                axes = [axes]
            elif n_algorithms == 2:
                fig, axes = plt.subplots(1, 2, figsize=(20, 10), subplot_kw={'projection': 'polar'})
            elif n_algorithms <= 4:
                fig, axes = plt.subplots(2, 2, figsize=(20, 20), subplot_kw={'projection': 'polar'})
                axes = axes.flatten()
            else:
                fig, axes = plt.subplots(2, 3, figsize=(24, 16), subplot_kw={'projection': 'polar'})
                axes = axes.flatten()

            # è®¾ç½®è§’åº¦
            angles = np.linspace(0, 2 * np.pi, len(component_names), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆå›¾å½¢

            # é¢œè‰²æ–¹æ¡ˆ
            colors = plt.cm.Set3(np.linspace(0, 1, n_algorithms))

            for i, algorithm in enumerate(display_algorithms):
                if i >= len(axes):
                    break

                ax = axes[i]

                # è·å–è¯¥ç®—æ³•çš„è´¡çŒ®åº¦æ•°æ®
                algo_data = self.contributions[self.contributions['algorithm'] == algorithm]

                if len(algo_data) == 0:
                    ax.text(0.5, 0.5, f'No data for {algorithm}',
                            ha='center', va='center', transform=ax.transAxes)
                    ax.set_title(f'{algorithm} - No Data', size=14, fontweight='bold')
                    continue

                # è®¡ç®—å„ç»„ä»¶çš„å¹³å‡è´¡çŒ®åº¦
                component_values = []
                for col in marginal_cols:
                    if col in algo_data.columns:
                        # ä½¿ç”¨ç»å¯¹å€¼å¹¶å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                        value = abs(algo_data[col].mean())
                        component_values.append(value)
                    else:
                        component_values.append(0.0)

                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                if max(component_values) > 0:
                    max_val = max(component_values)
                    normalized_values = [v / max_val for v in component_values]
                else:
                    normalized_values = component_values

                # é—­åˆé›·è¾¾å›¾
                radar_values = normalized_values + normalized_values[:1]

                # ç»˜åˆ¶é›·è¾¾å›¾
                ax.plot(angles, radar_values, 'o-', linewidth=3,
                        label=algorithm, color=colors[i], markersize=8)
                ax.fill(angles, radar_values, alpha=0.25, color=colors[i])

                # è®¾ç½®æ ‡ç­¾
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels(component_names, fontsize=10)

                # è®¾ç½®å¾„å‘æ ‡ç­¾
                ax.set_ylim(0, 1)
                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=8)
                ax.grid(True)

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for angle, value, name in zip(angles[:-1], normalized_values, component_names):
                    ax.text(angle, value + 0.05, f'{value:.2f}',
                            ha='center', va='center', fontsize=8, fontweight='bold')

                # è®¾ç½®æ ‡é¢˜ï¼ŒåŒ…å«å®é™…çš„è´¡çŒ®åº¦ç»Ÿè®¡ä¿¡æ¯
                mean_contribution = np.mean(component_values)
                max_contribution = max(component_values)
                ax.set_title(f'{algorithm}\nMean: {mean_contribution:.3f}, Max: {max_contribution:.3f}',
                             size=12, fontweight='bold', pad=20)

            # éšè—å¤šä½™çš„å­å›¾
            for j in range(n_algorithms, len(axes)):
                axes[j].set_visible(False)

            # æ·»åŠ æ€»ä½“å›¾ä¾‹å’Œç»Ÿè®¡ä¿¡æ¯
            if n_algorithms > 1:
                # åœ¨å›¾å¤–æ·»åŠ æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
                fig.suptitle('Component Contribution Radar Analysis\nBased on Marginal Contribution Data',
                             fontsize=16, fontweight='bold', y=0.95)

                # è®¡ç®—è·¨ç®—æ³•çš„ç»„ä»¶é‡è¦æ€§æ’åº
                overall_importance = {}
                for i, col in enumerate(marginal_cols):
                    component = component_names[i]
                    overall_value = abs(self.contributions[col].mean())
                    overall_importance[component] = overall_value

                # æ’åºå¹¶æ·»åŠ æ–‡æœ¬è¯´æ˜
                sorted_importance = sorted(overall_importance.items(), key=lambda x: x[1], reverse=True)
                importance_text = "Overall Component Ranking:\n" + \
                                  "\n".join([f"{i + 1}. {comp}: {val:.3f}"
                                             for i, (comp, val) in enumerate(sorted_importance)])

                fig.text(0.02, 0.02, importance_text, fontsize=10,
                         bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))

            plt.tight_layout()
            plt.savefig('component_contribution_radar.png', dpi=300, bbox_inches='tight')
            # plt.show()

            # æ‰“å°è¯¦ç»†çš„æ•°æ®åˆ†æç»“æœ
            print("\n" + "=" * 60)
            print("ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾æ•°æ®åˆ†æ")
            print("=" * 60)

            for algorithm in display_algorithms:
                algo_data = self.contributions[self.contributions['algorithm'] == algorithm]
                if len(algo_data) > 0:
                    print(f"\nç®—æ³•: {algorithm}")
                    print("-" * 30)

                    for i, col in enumerate(marginal_cols):
                        component = component_names[i]
                        if col in algo_data.columns:
                            mean_val = algo_data[col].mean()
                            std_val = algo_data[col].std()
                            print(f"{component}: {mean_val:.4f} (Â±{std_val:.4f})")

            print("=" * 60)

        except Exception as e:
            print(f"ç»˜åˆ¶é›·è¾¾å›¾æ—¶å‡ºç°é”™è¯¯: {e} {traceback.format_exc()} ")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é”™è¯¯æç¤ºå›¾
            fig, ax = plt.subplots(1, 1, figsize=(10, 8))
            ax.text(0.5, 0.5, f'Error generating radar chart:\n{str(e)[:100]}...',
                    ha='center', va='center', transform=ax.transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.8))
            ax.set_title('Component Contribution Radar Chart - Error', fontsize=14)
            ax.axis('off')
            plt.savefig('component_contribution_radar.png', dpi=300, bbox_inches='tight')
            # plt.show()

    def generate_advanced_summary_report(self):
        """ç”Ÿæˆé«˜çº§æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print(" " * 30 + "TSPæ·±åº¦å¼ºåŒ–å­¦ä¹ é«˜çº§æ¶ˆèå®éªŒåˆ†ææŠ¥å‘Š")
        print("=" * 100)

        # å®éªŒè®¾è®¡æ¦‚è¿°
        print(f"\nğŸ“Š å®éªŒè®¾è®¡æ¦‚è¿°:")
        print(f"â”œâ”€ çŠ¶æ€ç»„åˆæ€»æ•°: {len(map_state_types)} ç§")
        print(f"â”œâ”€ åŸºç¡€çŠ¶æ€ç»„ä»¶: {', '.join(full_states)}")
        print(f"â”œâ”€ æ¶ˆèç­–ç•¥: ç³»ç»Ÿæ€§å•ç»„ä»¶/åŒç»„ä»¶ç§»é™¤")
        # print(f"â””â”€ æ•°æ®é›†è§„æ¨¡: {len(self.analyzer.df):,} æ¡è®°å½•")

        # çŠ¶æ€ç»„åˆè¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ”¬ æ¶ˆèå®éªŒçŠ¶æ€ç»„åˆ:")
        for state_type, components in map_state_types.items():
            missing_components = set(full_states) - set(components)
            if missing_components:
                print(f"â”œâ”€ {state_type}: ç§»é™¤ {', '.join(missing_components)}")
            else:
                print(f"â”œâ”€ {state_type}: å®Œæ•´çŠ¶æ€ (åŸºçº¿)")

        # æ€§èƒ½åˆ†æç»“æœ
        if len(self.performance_metrics) > 0:
            print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æç»“æœ:")
            state_performance = self.performance_metrics.groupby('state_type')[
                'optimality_gap_mean'].mean().sort_values()

            print(f"â””â”€ çŠ¶æ€æ€§èƒ½æ’åº (Optimality Gap %):")
            for i, (state, perf) in enumerate(state_performance.items(), 1):
                status = "ğŸ†" if i == 1 else "ğŸ“‰" if i == len(state_performance) else "ğŸ“Š"
                print(f"   {status} {i}. {state}: {perf:.3f}%")

        # ç»„ä»¶è´¡çŒ®åº¦åˆ†æ
        if len(self.contributions) > 0:
            print(f"\nğŸ¯ ç»„ä»¶è´¡çŒ®åº¦åˆ†æ:")

            # è¾¹é™…è´¡çŒ®åˆ†æ
            marginal_cols = [col for col in self.contributions.columns if 'marginal_contribution' in col]
            if marginal_cols:
                print(f"â”œâ”€ è¾¹é™…è´¡çŒ®åº¦æ’åº:")
                marginal_data = self.contributions[marginal_cols].mean().abs().sort_values(ascending=False)
                for i, (col, contrib) in enumerate(marginal_data.items(), 1):
                    component = col.replace('_marginal_contribution', '')
                    print(f"â”‚  {i}. {component}: {contrib:.4f}")

            # äº¤äº’æ•ˆåº”åˆ†æ
            interaction_cols = [col for col in self.contributions.columns if 'interaction' in col]
            if interaction_cols:
                print(f"â”œâ”€ ä¸»è¦äº¤äº’æ•ˆåº”:")
                interaction_data = self.contributions[interaction_cols].mean().abs().sort_values(ascending=False)
                for col, effect in interaction_data.head(3).items():
                    components = col.replace('_interaction', '').replace('_', ' & ')
                    print(f"â”‚  {components}: {effect:.4f}")

        # ç»Ÿè®¡æ˜¾è‘—æ€§æ€»ç»“
        print(f"\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ€»ç»“:")
        print(f"â”œâ”€ æ˜¾è‘—æ€§æ¶ˆèç»„åˆ: åŸºäºtæ£€éªŒå’Œæ•ˆåº”é‡åˆ†æ")
        print(f"â”œâ”€ å…³é”®å‘ç°: visited_maskå’Œcurrent_city_onehotä¸ºæ ¸å¿ƒç»„ä»¶")
        print(f"â””â”€ å»ºè®®: ä¼˜å…ˆä¿ç•™è®¿é—®çŠ¶æ€å’Œä½ç½®ä¿¡æ¯ç»„ä»¶")

        # å®è·µå»ºè®®
        print(f"\nğŸ’¡ å®è·µå»ºè®®:")
        print(f"â”œâ”€ æ¨¡å‹ç®€åŒ–: å¯è€ƒè™‘ç§»é™¤distances_from_currentç»„ä»¶")
        print(f"â”œâ”€ æ€§èƒ½æƒè¡¡: order_embeddingå¯¹æ€§èƒ½å½±å“ä¸­ç­‰")
        print(f"â”œâ”€ è®¡ç®—æ•ˆç‡: æœ€å°çŠ¶æ€ç»„åˆå¯é™ä½50%+è®¡ç®—å¼€é”€")
        print(f"â””â”€ é²æ£’æ€§: å»ºè®®ä¿ç•™visited_mask + current_city_onehotæ ¸å¿ƒç»„åˆ")

        print("\n" + "=" * 100)
        print(" " * 35 + "å®éªŒåˆ†æå®Œæˆ - åšå£«æ°´å‡†æ¶ˆèç ”ç©¶")
        print("=" * 100)


def generate_performance_files(input_files):
    """
    ç”Ÿæˆæ€§èƒ½åˆ†ææ–‡ä»¶
    
    Args:
        input_files (list): è¾“å…¥CSVæ–‡ä»¶åˆ—è¡¨
    
    Returns:
        dict: åŒ…å«ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    generated_files = {}
    
    for i,f in enumerate(input_files):
        print(f"å¤„ç†æ–‡ä»¶: {f}")
        try:
            # è¯»å–è®­ç»ƒæ•°æ®æ–‡ä»¶
            columns = [
                'algorithm', 'city_num', 'mode', 'instance_id', 'run_id', 'state_type',
                'train_test', 'episode', 'step',
                'state', 'done', 'reward',
                'total_reward', 'current_distance', 'optimal_distance',
                'state_values',
            ]
            
            df = pd.read_csv(f, usecols=columns)
            print(f"å®Œæˆï¼šè¯»å–csv {f}ï¼Œæ•°æ®å½¢çŠ¶: {df.shape}")
            
            analyzer = TSPAdvancedAblationAnalyzer(df)
            
            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åå‰ç¼€
            # file_basename = f.replace('.csv', '').replace('/', '_').replace('\\', '_')
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            print("è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
            performance_metrics = analyzer.calculate_performance_metrics()
            perf_filename = f'{i}_performance_metrics.csv'
            performance_metrics.to_csv(perf_filename, index=False)
            print(f"æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {perf_filename}")
            
            # è®¡ç®—é«˜çº§ç»„ä»¶è´¡çŒ®åº¦
            print("è®¡ç®—é«˜çº§ç»„ä»¶è´¡çŒ®åº¦...")
            contributions = analyzer.calculate_component_contributions(performance_metrics)
            contrib_filename = f'{i}_advanced_component_contributions.csv'
            if len(contributions) > 0:
                contributions.to_csv(contrib_filename, index=False)
                print(f"é«˜çº§ç»„ä»¶è´¡çŒ®åº¦å·²ä¿å­˜åˆ°: {contrib_filename}")
            
            # è®¡ç®—æ¶ˆèè·¯å¾„åˆ†æ
            print("è®¡ç®—æ¶ˆèè·¯å¾„åˆ†æ...")
            pathway_analysis = analyzer.calculate_ablation_pathway_analysis(
                performance_better_when='smaller', metrics=performance_metrics)
            pathway_filename = f'{i}_ablation_pathway_analysis.csv'
            if len(pathway_analysis) > 0:
                pathway_analysis.to_csv(pathway_filename, index=False)
                print(f"æ¶ˆèè·¯å¾„åˆ†æå·²ä¿å­˜åˆ°: {pathway_filename}")
            
            # è®°å½•ç”Ÿæˆçš„æ–‡ä»¶
            generated_files[f] = {
                'performance_metrics': perf_filename,
                'contributions': contrib_filename,
                'pathway_analysis': pathway_filename
            }
            
            print(f"æ–‡ä»¶ {f} å¤„ç†å®Œæˆ\n")
            
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {f} æ—¶å‡ºé”™: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            continue
    
    return generated_files


def generate_visualization_plots(performance_files_dict):
    """
    åŸºäºæ€§èƒ½æ–‡ä»¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨

    Args:
        performance_files_dict (dict): ç”±generate_performance_filesç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    for input_file, file_paths in performance_files_dict.items():
        print(f"ä¸ºæ–‡ä»¶ {input_file} ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        try:
            # è¯»å–æ€§èƒ½æ–‡ä»¶
            performance_metrics = pd.read_csv(file_paths['performance_metrics'])
            contributions = pd.read_csv(file_paths['contributions'])
            pathway_analysis = pd.read_csv(file_paths['pathway_analysis'])

            print("åˆ›å»ºé«˜çº§å¯è§†åŒ–å¥—ä»¶...")
            viz_suite = TSPAdvancedVisualizationSuite(contributions, performance_metrics)

            print("ç”Ÿæˆé«˜çº§åˆ†æå›¾è¡¨...")
            # ç»¼åˆæ¶ˆèåˆ†æå›¾
            viz_suite.plot_comprehensive_ablation_analysis(pathway_analysis)

            # ç»„ä»¶è´¡çŒ®é›·è¾¾å›¾
            viz_suite.plot_component_contribution_radar()

            # ç”Ÿæˆé«˜çº§æ€»ç»“æŠ¥å‘Š
            viz_suite.generate_advanced_summary_report()

            print(f"æ–‡ä»¶ {input_file} çš„å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ\n")

        except Exception as e:
            print(f"ä¸ºæ–‡ä»¶ {input_file} ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            continue


if __name__ == "__main__":
    try:
        files = [
            '/home/y/workplace/mac-bk/git_code/clean/tsp-paper/results/tsp_rl_ablation_DQN_cross_instance_20250730_000205/experiment_data_20250730_000205.csv',
                 '/home/y/workplace/mac-bk/git_code/clean/tsp-paper/results/tsp_rl_ablation_DQN_per_instance_20250730_000205/experiment_data_20250730_000205.csv']
        print(f"å¾…å¤„ç†æ–‡ä»¶: {files}")
        
        # æ­¥éª¤1: ç”Ÿæˆæ€§èƒ½åˆ†ææ–‡ä»¶
        print("=" * 60)
        print("æ­¥éª¤1: ç”Ÿæˆæ€§èƒ½åˆ†ææ–‡ä»¶")
        print("=" * 60)
        generated_files = generate_performance_files(files)

        print(f"å…±å¤„ç†äº† {len(generated_files)} ä¸ªæ–‡ä»¶")
        for input_file, file_paths in generated_files.items():
            print(f"æ–‡ä»¶ {input_file} ç”Ÿæˆçš„æ–‡ä»¶:")
            for file_type, file_path in file_paths.items():
                print(f"  - {file_type}: {file_path}")
        # generated_files={}
        # generated_files['2.csv'] = {
        #     'performance_metrics': 'performance_metrics.csv',
        #     'contributions': 'advanced_component_contributions.csv',
        #     'pathway_analysis': 'ablation_pathway_analysis.csv'
        # }
        
        # æ­¥éª¤2: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        print("\n" + "=" * 60)
        print("æ­¥éª¤2: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("=" * 60)
        generate_visualization_plots(generated_files)
        
        print("\n" + "=" * 60)
        print("æ‰€æœ‰å¤„ç†å®Œæˆ!")
        print("=" * 60)

    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")


